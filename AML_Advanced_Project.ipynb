{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AML_Advanced_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nFJYC8oteS0I",
        "5umeV4M-vqC5",
        "fecVrOzjsy2j",
        "C5GsTUg-wCij",
        "sa0xg2P772cN",
        "61S3Py86TR8K",
        "lTey7K62WB_W",
        "TVVw7xcL6xaj",
        "dF6fWoPmJDIQ",
        "Z-DT9pDnH6CA",
        "3UTJ783zMvKb",
        "pr_tjYn3M2aN",
        "GNABt2jE9Vf0",
        "Gk1CbOLS1Lip",
        "uTcjaTMj0HxT"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHokli3anuXA"
      },
      "source": [
        "###**Advanced Machine Learning - Edoardo Giuili** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPlKIhiOeBIT"
      },
      "source": [
        "# **Gene expression analysis for colorectal cancer prediction**\n",
        "The dataset can be found at the following site: https://sbcb.inf.ufrgs.br/cumida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWBv9mlDovEF"
      },
      "source": [
        "## **Description of the DataSet**\n",
        "The Curated Microarray Database (CuMiDa) is a repository containing  several cancer microarray datasets. CuMiDa offers new, manually and carefully curated datasets (*Feltes, B.C.; et al. CuMiDa: An Extensively Curated Microarray Database for Benchmarking and Testing of Machine Learning Approaches in Cancer Research. Journal of Computational Biology, 2019*). \n",
        "In my case, the dataset contains 105 rows/samples (patients) and 22280 columns: 2 columns represent the sample id and the categorical value (tumoral or normal) while the other columns represent the expression of 22278 different genes. \n",
        "The aim of this project is to design one or more machine learning methods that are able to make accurate predictions on new unknown examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preprocessing**"
      ],
      "metadata": {
        "id": "DUAgbSMo7Vq3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJYC8oteS0I"
      },
      "source": [
        "## *Step 1 - Import the Libraries*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws8GLX4Ydv7W"
      },
      "source": [
        "# Import General Libraries\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Import Libraries for Classification\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sklearn.metrics \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Import Libraries for Deep Learning\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow import keras\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5umeV4M-vqC5"
      },
      "source": [
        "## *Step 2 - Load Data and process it*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "WcIVVAmzvvGn",
        "outputId": "0a31c8f9-e674-444b-c924-d79242ef2027"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a821dc33-d5e1-46d9-9082-061d33712755\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a821dc33-d5e1-46d9-9082-061d33712755\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Colorectal_GSE44861.csv to Colorectal_GSE44861.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "ILTl8t0gFB0J",
        "outputId": "3ccc424a-aac1-4e9d-a294-6619069e9c74"
      },
      "source": [
        "entire_ds = pd.read_csv(\"drive/MyDrive/Colorectal_GSE44861.csv\")\n",
        "print(entire_ds.shape)\n",
        "entire_ds.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105, 22279)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>samples</th>\n",
              "      <th>type</th>\n",
              "      <th>1007_s_at</th>\n",
              "      <th>1053_at</th>\n",
              "      <th>117_at</th>\n",
              "      <th>121_at</th>\n",
              "      <th>1255_g_at</th>\n",
              "      <th>1294_at</th>\n",
              "      <th>1316_at</th>\n",
              "      <th>1320_at</th>\n",
              "      <th>1405_i_at</th>\n",
              "      <th>1431_at</th>\n",
              "      <th>1438_at</th>\n",
              "      <th>1487_at</th>\n",
              "      <th>1494_f_at</th>\n",
              "      <th>1598_g_at</th>\n",
              "      <th>160020_at</th>\n",
              "      <th>1729_at</th>\n",
              "      <th>1773_at</th>\n",
              "      <th>177_at</th>\n",
              "      <th>179_at</th>\n",
              "      <th>1861_at</th>\n",
              "      <th>200000_s_at</th>\n",
              "      <th>200001_at</th>\n",
              "      <th>200002_at</th>\n",
              "      <th>200003_s_at</th>\n",
              "      <th>200004_at</th>\n",
              "      <th>200005_at</th>\n",
              "      <th>200006_at</th>\n",
              "      <th>200007_at</th>\n",
              "      <th>200008_s_at</th>\n",
              "      <th>200009_at</th>\n",
              "      <th>200010_at</th>\n",
              "      <th>200011_s_at</th>\n",
              "      <th>200012_x_at</th>\n",
              "      <th>200013_at</th>\n",
              "      <th>200014_s_at</th>\n",
              "      <th>200015_s_at</th>\n",
              "      <th>200016_x_at</th>\n",
              "      <th>200017_at</th>\n",
              "      <th>...</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_MB_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_3_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_5_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_M_at</th>\n",
              "      <th>AFFX-LysX-3_at</th>\n",
              "      <th>AFFX-LysX-5_at</th>\n",
              "      <th>AFFX-LysX-M_at</th>\n",
              "      <th>AFFX-M27830_3_at</th>\n",
              "      <th>AFFX-M27830_5_at</th>\n",
              "      <th>AFFX-M27830_M_at</th>\n",
              "      <th>AFFX-PheX-3_at</th>\n",
              "      <th>AFFX-PheX-5_at</th>\n",
              "      <th>AFFX-PheX-M_at</th>\n",
              "      <th>AFFX-r2-Bs-dap-3_at</th>\n",
              "      <th>AFFX-r2-Bs-dap-5_at</th>\n",
              "      <th>AFFX-r2-Bs-dap-M_at</th>\n",
              "      <th>AFFX-r2-Bs-lys-3_at</th>\n",
              "      <th>AFFX-r2-Bs-lys-5_at</th>\n",
              "      <th>AFFX-r2-Bs-lys-M_at</th>\n",
              "      <th>AFFX-r2-Bs-phe-3_at</th>\n",
              "      <th>AFFX-r2-Bs-phe-5_at</th>\n",
              "      <th>AFFX-r2-Bs-phe-M_at</th>\n",
              "      <th>AFFX-r2-Bs-thr-3_s_at</th>\n",
              "      <th>AFFX-r2-Bs-thr-5_s_at</th>\n",
              "      <th>AFFX-r2-Bs-thr-M_s_at</th>\n",
              "      <th>AFFX-r2-Ec-bioB-3_at</th>\n",
              "      <th>AFFX-r2-Ec-bioB-5_at</th>\n",
              "      <th>AFFX-r2-Ec-bioB-M_at</th>\n",
              "      <th>AFFX-r2-Ec-bioC-3_at</th>\n",
              "      <th>AFFX-r2-Ec-bioC-5_at</th>\n",
              "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
              "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
              "      <th>AFFX-r2-P1-cre-3_at</th>\n",
              "      <th>AFFX-r2-P1-cre-5_at</th>\n",
              "      <th>AFFX-ThrX-3_at</th>\n",
              "      <th>AFFX-ThrX-5_at</th>\n",
              "      <th>AFFX-ThrX-M_at</th>\n",
              "      <th>AFFX-TrpnX-3_at</th>\n",
              "      <th>AFFX-TrpnX-5_at</th>\n",
              "      <th>AFFX-TrpnX-M_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>909</td>\n",
              "      <td>normal</td>\n",
              "      <td>11.603630</td>\n",
              "      <td>6.161494</td>\n",
              "      <td>5.586689</td>\n",
              "      <td>7.665427</td>\n",
              "      <td>5.181192</td>\n",
              "      <td>9.328589</td>\n",
              "      <td>5.667076</td>\n",
              "      <td>5.825957</td>\n",
              "      <td>7.684897</td>\n",
              "      <td>5.280453</td>\n",
              "      <td>5.924691</td>\n",
              "      <td>9.171197</td>\n",
              "      <td>5.892172</td>\n",
              "      <td>10.675036</td>\n",
              "      <td>7.130390</td>\n",
              "      <td>8.365860</td>\n",
              "      <td>5.777034</td>\n",
              "      <td>7.294819</td>\n",
              "      <td>7.452211</td>\n",
              "      <td>7.183493</td>\n",
              "      <td>10.563175</td>\n",
              "      <td>11.694286</td>\n",
              "      <td>11.793642</td>\n",
              "      <td>14.133192</td>\n",
              "      <td>13.253494</td>\n",
              "      <td>11.688668</td>\n",
              "      <td>12.730797</td>\n",
              "      <td>13.111323</td>\n",
              "      <td>10.624835</td>\n",
              "      <td>13.324794</td>\n",
              "      <td>13.509437</td>\n",
              "      <td>11.613050</td>\n",
              "      <td>14.513084</td>\n",
              "      <td>13.557289</td>\n",
              "      <td>11.010666</td>\n",
              "      <td>12.461846</td>\n",
              "      <td>13.717629</td>\n",
              "      <td>14.266796</td>\n",
              "      <td>...</td>\n",
              "      <td>7.421438</td>\n",
              "      <td>12.472236</td>\n",
              "      <td>12.823305</td>\n",
              "      <td>11.611549</td>\n",
              "      <td>5.473242</td>\n",
              "      <td>5.038363</td>\n",
              "      <td>5.336605</td>\n",
              "      <td>5.434170</td>\n",
              "      <td>12.581952</td>\n",
              "      <td>10.383143</td>\n",
              "      <td>5.850063</td>\n",
              "      <td>5.218544</td>\n",
              "      <td>4.953496</td>\n",
              "      <td>4.927370</td>\n",
              "      <td>5.013688</td>\n",
              "      <td>4.972475</td>\n",
              "      <td>5.332326</td>\n",
              "      <td>5.216208</td>\n",
              "      <td>4.976444</td>\n",
              "      <td>5.588318</td>\n",
              "      <td>5.116983</td>\n",
              "      <td>5.207860</td>\n",
              "      <td>5.128672</td>\n",
              "      <td>5.333412</td>\n",
              "      <td>5.202440</td>\n",
              "      <td>9.222836</td>\n",
              "      <td>8.555462</td>\n",
              "      <td>8.541967</td>\n",
              "      <td>9.949472</td>\n",
              "      <td>10.225310</td>\n",
              "      <td>12.825918</td>\n",
              "      <td>12.744079</td>\n",
              "      <td>14.633211</td>\n",
              "      <td>14.077696</td>\n",
              "      <td>5.493690</td>\n",
              "      <td>5.129016</td>\n",
              "      <td>5.013772</td>\n",
              "      <td>4.964384</td>\n",
              "      <td>5.012729</td>\n",
              "      <td>5.032029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>911</td>\n",
              "      <td>normal</td>\n",
              "      <td>10.724242</td>\n",
              "      <td>6.168925</td>\n",
              "      <td>5.645848</td>\n",
              "      <td>8.285704</td>\n",
              "      <td>5.270711</td>\n",
              "      <td>8.892988</td>\n",
              "      <td>6.484289</td>\n",
              "      <td>5.596518</td>\n",
              "      <td>7.735154</td>\n",
              "      <td>5.680837</td>\n",
              "      <td>5.373245</td>\n",
              "      <td>9.374720</td>\n",
              "      <td>6.469610</td>\n",
              "      <td>10.410668</td>\n",
              "      <td>7.434784</td>\n",
              "      <td>8.037932</td>\n",
              "      <td>5.692502</td>\n",
              "      <td>7.650652</td>\n",
              "      <td>8.252928</td>\n",
              "      <td>6.130473</td>\n",
              "      <td>9.415151</td>\n",
              "      <td>9.691296</td>\n",
              "      <td>10.760481</td>\n",
              "      <td>13.053457</td>\n",
              "      <td>12.786984</td>\n",
              "      <td>10.893364</td>\n",
              "      <td>11.531514</td>\n",
              "      <td>12.790593</td>\n",
              "      <td>7.645248</td>\n",
              "      <td>12.030956</td>\n",
              "      <td>12.254362</td>\n",
              "      <td>10.844536</td>\n",
              "      <td>14.126348</td>\n",
              "      <td>12.373588</td>\n",
              "      <td>10.444607</td>\n",
              "      <td>12.200934</td>\n",
              "      <td>13.551583</td>\n",
              "      <td>13.896925</td>\n",
              "      <td>...</td>\n",
              "      <td>6.336246</td>\n",
              "      <td>12.044913</td>\n",
              "      <td>13.853550</td>\n",
              "      <td>12.724685</td>\n",
              "      <td>5.728112</td>\n",
              "      <td>5.380797</td>\n",
              "      <td>5.343777</td>\n",
              "      <td>5.905650</td>\n",
              "      <td>12.531953</td>\n",
              "      <td>11.676500</td>\n",
              "      <td>6.545668</td>\n",
              "      <td>5.276944</td>\n",
              "      <td>5.160995</td>\n",
              "      <td>5.000274</td>\n",
              "      <td>5.093408</td>\n",
              "      <td>5.191535</td>\n",
              "      <td>5.537860</td>\n",
              "      <td>5.232726</td>\n",
              "      <td>5.146406</td>\n",
              "      <td>5.845758</td>\n",
              "      <td>6.267728</td>\n",
              "      <td>5.106717</td>\n",
              "      <td>5.253862</td>\n",
              "      <td>5.393300</td>\n",
              "      <td>5.259405</td>\n",
              "      <td>11.920945</td>\n",
              "      <td>11.540363</td>\n",
              "      <td>11.581186</td>\n",
              "      <td>12.507296</td>\n",
              "      <td>12.877094</td>\n",
              "      <td>14.610437</td>\n",
              "      <td>14.852044</td>\n",
              "      <td>15.309681</td>\n",
              "      <td>15.190121</td>\n",
              "      <td>5.535807</td>\n",
              "      <td>5.063369</td>\n",
              "      <td>5.103648</td>\n",
              "      <td>5.089872</td>\n",
              "      <td>5.156410</td>\n",
              "      <td>5.270410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>913</td>\n",
              "      <td>normal</td>\n",
              "      <td>9.897182</td>\n",
              "      <td>6.141052</td>\n",
              "      <td>6.028690</td>\n",
              "      <td>7.382975</td>\n",
              "      <td>5.241439</td>\n",
              "      <td>8.906832</td>\n",
              "      <td>6.253771</td>\n",
              "      <td>5.728508</td>\n",
              "      <td>8.553831</td>\n",
              "      <td>5.306472</td>\n",
              "      <td>5.497906</td>\n",
              "      <td>8.239426</td>\n",
              "      <td>5.861003</td>\n",
              "      <td>10.149764</td>\n",
              "      <td>7.203162</td>\n",
              "      <td>8.205351</td>\n",
              "      <td>5.767240</td>\n",
              "      <td>6.306600</td>\n",
              "      <td>7.136410</td>\n",
              "      <td>6.118094</td>\n",
              "      <td>10.557734</td>\n",
              "      <td>9.942647</td>\n",
              "      <td>12.476845</td>\n",
              "      <td>14.577318</td>\n",
              "      <td>12.884134</td>\n",
              "      <td>11.877103</td>\n",
              "      <td>13.159349</td>\n",
              "      <td>13.158277</td>\n",
              "      <td>8.516653</td>\n",
              "      <td>12.981577</td>\n",
              "      <td>13.543751</td>\n",
              "      <td>10.236101</td>\n",
              "      <td>14.826589</td>\n",
              "      <td>13.889126</td>\n",
              "      <td>11.686800</td>\n",
              "      <td>12.700132</td>\n",
              "      <td>14.270469</td>\n",
              "      <td>14.620751</td>\n",
              "      <td>...</td>\n",
              "      <td>6.072724</td>\n",
              "      <td>12.921625</td>\n",
              "      <td>13.395459</td>\n",
              "      <td>12.750573</td>\n",
              "      <td>5.588481</td>\n",
              "      <td>5.231683</td>\n",
              "      <td>5.432931</td>\n",
              "      <td>5.697047</td>\n",
              "      <td>12.539151</td>\n",
              "      <td>10.797103</td>\n",
              "      <td>6.130881</td>\n",
              "      <td>5.343271</td>\n",
              "      <td>5.036490</td>\n",
              "      <td>5.092366</td>\n",
              "      <td>5.098212</td>\n",
              "      <td>5.090961</td>\n",
              "      <td>5.207550</td>\n",
              "      <td>4.967397</td>\n",
              "      <td>5.024144</td>\n",
              "      <td>5.471654</td>\n",
              "      <td>5.447919</td>\n",
              "      <td>5.363319</td>\n",
              "      <td>5.487234</td>\n",
              "      <td>5.585319</td>\n",
              "      <td>5.145920</td>\n",
              "      <td>10.562255</td>\n",
              "      <td>10.171149</td>\n",
              "      <td>10.124445</td>\n",
              "      <td>11.320522</td>\n",
              "      <td>11.639455</td>\n",
              "      <td>13.795802</td>\n",
              "      <td>14.016776</td>\n",
              "      <td>15.093576</td>\n",
              "      <td>14.821565</td>\n",
              "      <td>5.406839</td>\n",
              "      <td>5.186247</td>\n",
              "      <td>5.113486</td>\n",
              "      <td>5.021668</td>\n",
              "      <td>5.170374</td>\n",
              "      <td>5.031002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>916</td>\n",
              "      <td>normal</td>\n",
              "      <td>10.177590</td>\n",
              "      <td>6.547922</td>\n",
              "      <td>5.657623</td>\n",
              "      <td>8.108889</td>\n",
              "      <td>5.309596</td>\n",
              "      <td>9.694124</td>\n",
              "      <td>6.511937</td>\n",
              "      <td>5.666979</td>\n",
              "      <td>7.737384</td>\n",
              "      <td>5.845548</td>\n",
              "      <td>5.902324</td>\n",
              "      <td>8.407829</td>\n",
              "      <td>6.315867</td>\n",
              "      <td>10.328124</td>\n",
              "      <td>7.244356</td>\n",
              "      <td>8.320469</td>\n",
              "      <td>6.045240</td>\n",
              "      <td>6.019156</td>\n",
              "      <td>7.755093</td>\n",
              "      <td>6.218387</td>\n",
              "      <td>10.711764</td>\n",
              "      <td>10.561821</td>\n",
              "      <td>12.321879</td>\n",
              "      <td>14.567547</td>\n",
              "      <td>12.159402</td>\n",
              "      <td>12.169462</td>\n",
              "      <td>12.772461</td>\n",
              "      <td>12.678296</td>\n",
              "      <td>8.753287</td>\n",
              "      <td>12.975418</td>\n",
              "      <td>13.677728</td>\n",
              "      <td>10.650770</td>\n",
              "      <td>14.813610</td>\n",
              "      <td>13.609111</td>\n",
              "      <td>11.203525</td>\n",
              "      <td>12.341656</td>\n",
              "      <td>14.170792</td>\n",
              "      <td>14.321302</td>\n",
              "      <td>...</td>\n",
              "      <td>6.035609</td>\n",
              "      <td>12.967256</td>\n",
              "      <td>13.918037</td>\n",
              "      <td>13.399531</td>\n",
              "      <td>5.883299</td>\n",
              "      <td>5.174830</td>\n",
              "      <td>5.548479</td>\n",
              "      <td>6.323225</td>\n",
              "      <td>14.013843</td>\n",
              "      <td>11.683140</td>\n",
              "      <td>6.000761</td>\n",
              "      <td>5.408122</td>\n",
              "      <td>5.129871</td>\n",
              "      <td>5.250453</td>\n",
              "      <td>5.101656</td>\n",
              "      <td>5.185830</td>\n",
              "      <td>5.422394</td>\n",
              "      <td>5.195827</td>\n",
              "      <td>5.009103</td>\n",
              "      <td>6.256389</td>\n",
              "      <td>6.254604</td>\n",
              "      <td>5.279590</td>\n",
              "      <td>5.314941</td>\n",
              "      <td>5.717720</td>\n",
              "      <td>5.407801</td>\n",
              "      <td>11.899847</td>\n",
              "      <td>11.541538</td>\n",
              "      <td>11.518942</td>\n",
              "      <td>12.586304</td>\n",
              "      <td>12.969694</td>\n",
              "      <td>14.516896</td>\n",
              "      <td>14.672769</td>\n",
              "      <td>15.197588</td>\n",
              "      <td>15.091035</td>\n",
              "      <td>5.547692</td>\n",
              "      <td>5.284327</td>\n",
              "      <td>5.242697</td>\n",
              "      <td>5.357589</td>\n",
              "      <td>5.248072</td>\n",
              "      <td>5.063287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>917</td>\n",
              "      <td>normal</td>\n",
              "      <td>10.243669</td>\n",
              "      <td>5.703212</td>\n",
              "      <td>5.644889</td>\n",
              "      <td>8.296944</td>\n",
              "      <td>5.542044</td>\n",
              "      <td>9.384085</td>\n",
              "      <td>6.967511</td>\n",
              "      <td>5.681129</td>\n",
              "      <td>8.910061</td>\n",
              "      <td>5.683377</td>\n",
              "      <td>6.097692</td>\n",
              "      <td>8.359901</td>\n",
              "      <td>6.376017</td>\n",
              "      <td>10.373152</td>\n",
              "      <td>7.756876</td>\n",
              "      <td>8.323301</td>\n",
              "      <td>6.000281</td>\n",
              "      <td>6.775999</td>\n",
              "      <td>8.007685</td>\n",
              "      <td>5.963374</td>\n",
              "      <td>8.455941</td>\n",
              "      <td>7.761780</td>\n",
              "      <td>11.030542</td>\n",
              "      <td>12.784515</td>\n",
              "      <td>11.697208</td>\n",
              "      <td>10.735593</td>\n",
              "      <td>12.222912</td>\n",
              "      <td>12.705972</td>\n",
              "      <td>7.802126</td>\n",
              "      <td>10.617630</td>\n",
              "      <td>12.515660</td>\n",
              "      <td>9.423400</td>\n",
              "      <td>14.734102</td>\n",
              "      <td>12.739837</td>\n",
              "      <td>10.803399</td>\n",
              "      <td>12.876395</td>\n",
              "      <td>14.264361</td>\n",
              "      <td>14.413956</td>\n",
              "      <td>...</td>\n",
              "      <td>6.481728</td>\n",
              "      <td>10.114141</td>\n",
              "      <td>10.585099</td>\n",
              "      <td>10.986814</td>\n",
              "      <td>6.234042</td>\n",
              "      <td>5.373069</td>\n",
              "      <td>5.552485</td>\n",
              "      <td>6.138474</td>\n",
              "      <td>10.613065</td>\n",
              "      <td>10.374428</td>\n",
              "      <td>6.048980</td>\n",
              "      <td>5.575283</td>\n",
              "      <td>5.415161</td>\n",
              "      <td>5.379466</td>\n",
              "      <td>5.321983</td>\n",
              "      <td>5.523485</td>\n",
              "      <td>6.010949</td>\n",
              "      <td>5.383782</td>\n",
              "      <td>5.141887</td>\n",
              "      <td>6.137183</td>\n",
              "      <td>5.808421</td>\n",
              "      <td>5.303310</td>\n",
              "      <td>5.421791</td>\n",
              "      <td>5.581629</td>\n",
              "      <td>5.613864</td>\n",
              "      <td>12.101202</td>\n",
              "      <td>11.590230</td>\n",
              "      <td>11.492097</td>\n",
              "      <td>12.522723</td>\n",
              "      <td>12.878605</td>\n",
              "      <td>14.572370</td>\n",
              "      <td>14.688109</td>\n",
              "      <td>15.174612</td>\n",
              "      <td>15.069524</td>\n",
              "      <td>5.812392</td>\n",
              "      <td>5.555466</td>\n",
              "      <td>5.185894</td>\n",
              "      <td>5.402853</td>\n",
              "      <td>5.236353</td>\n",
              "      <td>5.356429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>920</td>\n",
              "      <td>normal</td>\n",
              "      <td>10.883507</td>\n",
              "      <td>6.358084</td>\n",
              "      <td>5.713601</td>\n",
              "      <td>7.798066</td>\n",
              "      <td>5.290464</td>\n",
              "      <td>8.931954</td>\n",
              "      <td>5.914310</td>\n",
              "      <td>5.744955</td>\n",
              "      <td>8.721526</td>\n",
              "      <td>5.337788</td>\n",
              "      <td>5.858222</td>\n",
              "      <td>9.292434</td>\n",
              "      <td>5.735491</td>\n",
              "      <td>11.040338</td>\n",
              "      <td>6.744978</td>\n",
              "      <td>8.466489</td>\n",
              "      <td>5.752464</td>\n",
              "      <td>7.202450</td>\n",
              "      <td>7.046610</td>\n",
              "      <td>6.713187</td>\n",
              "      <td>10.212622</td>\n",
              "      <td>11.138651</td>\n",
              "      <td>12.057615</td>\n",
              "      <td>14.119232</td>\n",
              "      <td>12.763285</td>\n",
              "      <td>11.750927</td>\n",
              "      <td>12.784855</td>\n",
              "      <td>12.930344</td>\n",
              "      <td>8.295308</td>\n",
              "      <td>12.820016</td>\n",
              "      <td>13.470702</td>\n",
              "      <td>11.339424</td>\n",
              "      <td>14.580265</td>\n",
              "      <td>13.517375</td>\n",
              "      <td>11.180746</td>\n",
              "      <td>12.520592</td>\n",
              "      <td>14.028707</td>\n",
              "      <td>14.354729</td>\n",
              "      <td>...</td>\n",
              "      <td>6.052173</td>\n",
              "      <td>11.833752</td>\n",
              "      <td>12.402697</td>\n",
              "      <td>11.054072</td>\n",
              "      <td>5.573618</td>\n",
              "      <td>5.156082</td>\n",
              "      <td>5.420193</td>\n",
              "      <td>5.522676</td>\n",
              "      <td>12.227169</td>\n",
              "      <td>10.166459</td>\n",
              "      <td>5.909235</td>\n",
              "      <td>5.070872</td>\n",
              "      <td>4.977044</td>\n",
              "      <td>5.025551</td>\n",
              "      <td>4.851883</td>\n",
              "      <td>5.017461</td>\n",
              "      <td>5.222631</td>\n",
              "      <td>5.253409</td>\n",
              "      <td>5.126335</td>\n",
              "      <td>5.343424</td>\n",
              "      <td>5.143775</td>\n",
              "      <td>5.324338</td>\n",
              "      <td>5.151965</td>\n",
              "      <td>5.726868</td>\n",
              "      <td>5.085027</td>\n",
              "      <td>9.545492</td>\n",
              "      <td>9.129389</td>\n",
              "      <td>9.147066</td>\n",
              "      <td>10.246157</td>\n",
              "      <td>10.559741</td>\n",
              "      <td>12.954691</td>\n",
              "      <td>12.989804</td>\n",
              "      <td>14.655947</td>\n",
              "      <td>14.266794</td>\n",
              "      <td>5.466816</td>\n",
              "      <td>5.072277</td>\n",
              "      <td>5.231908</td>\n",
              "      <td>5.096166</td>\n",
              "      <td>5.068430</td>\n",
              "      <td>4.949972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>922</td>\n",
              "      <td>normal</td>\n",
              "      <td>10.334476</td>\n",
              "      <td>6.503902</td>\n",
              "      <td>6.225749</td>\n",
              "      <td>7.837346</td>\n",
              "      <td>5.108317</td>\n",
              "      <td>8.408111</td>\n",
              "      <td>5.864856</td>\n",
              "      <td>5.482676</td>\n",
              "      <td>8.351655</td>\n",
              "      <td>5.496987</td>\n",
              "      <td>5.627820</td>\n",
              "      <td>8.965381</td>\n",
              "      <td>5.797710</td>\n",
              "      <td>10.744541</td>\n",
              "      <td>6.907291</td>\n",
              "      <td>8.392720</td>\n",
              "      <td>5.617682</td>\n",
              "      <td>6.538549</td>\n",
              "      <td>7.099215</td>\n",
              "      <td>6.516085</td>\n",
              "      <td>9.839878</td>\n",
              "      <td>10.643651</td>\n",
              "      <td>12.339436</td>\n",
              "      <td>14.352482</td>\n",
              "      <td>12.872265</td>\n",
              "      <td>11.764884</td>\n",
              "      <td>12.994810</td>\n",
              "      <td>13.157023</td>\n",
              "      <td>8.490657</td>\n",
              "      <td>12.609861</td>\n",
              "      <td>13.633622</td>\n",
              "      <td>10.835648</td>\n",
              "      <td>14.741516</td>\n",
              "      <td>13.830141</td>\n",
              "      <td>11.296805</td>\n",
              "      <td>12.636087</td>\n",
              "      <td>14.039177</td>\n",
              "      <td>14.530964</td>\n",
              "      <td>...</td>\n",
              "      <td>6.133281</td>\n",
              "      <td>11.401690</td>\n",
              "      <td>13.041445</td>\n",
              "      <td>11.449972</td>\n",
              "      <td>5.387394</td>\n",
              "      <td>5.149367</td>\n",
              "      <td>5.197701</td>\n",
              "      <td>5.452211</td>\n",
              "      <td>11.940227</td>\n",
              "      <td>10.513452</td>\n",
              "      <td>5.927119</td>\n",
              "      <td>5.164437</td>\n",
              "      <td>5.005276</td>\n",
              "      <td>4.937751</td>\n",
              "      <td>4.931643</td>\n",
              "      <td>4.978742</td>\n",
              "      <td>5.333754</td>\n",
              "      <td>5.074768</td>\n",
              "      <td>5.073527</td>\n",
              "      <td>5.575022</td>\n",
              "      <td>5.174150</td>\n",
              "      <td>5.205865</td>\n",
              "      <td>5.323120</td>\n",
              "      <td>5.456077</td>\n",
              "      <td>5.279505</td>\n",
              "      <td>10.306641</td>\n",
              "      <td>9.835792</td>\n",
              "      <td>9.835504</td>\n",
              "      <td>10.966547</td>\n",
              "      <td>11.329361</td>\n",
              "      <td>13.616817</td>\n",
              "      <td>13.655340</td>\n",
              "      <td>14.934272</td>\n",
              "      <td>14.633959</td>\n",
              "      <td>5.347464</td>\n",
              "      <td>5.044476</td>\n",
              "      <td>4.941938</td>\n",
              "      <td>5.103553</td>\n",
              "      <td>4.970727</td>\n",
              "      <td>4.974458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>924</td>\n",
              "      <td>normal</td>\n",
              "      <td>11.104597</td>\n",
              "      <td>6.712157</td>\n",
              "      <td>5.668190</td>\n",
              "      <td>7.373610</td>\n",
              "      <td>5.238014</td>\n",
              "      <td>8.807724</td>\n",
              "      <td>5.585496</td>\n",
              "      <td>5.794391</td>\n",
              "      <td>10.563897</td>\n",
              "      <td>5.376059</td>\n",
              "      <td>6.485171</td>\n",
              "      <td>8.862814</td>\n",
              "      <td>5.837258</td>\n",
              "      <td>11.035745</td>\n",
              "      <td>7.074368</td>\n",
              "      <td>8.043208</td>\n",
              "      <td>5.732058</td>\n",
              "      <td>6.570883</td>\n",
              "      <td>7.293883</td>\n",
              "      <td>6.624913</td>\n",
              "      <td>11.021810</td>\n",
              "      <td>11.374629</td>\n",
              "      <td>12.208189</td>\n",
              "      <td>14.473121</td>\n",
              "      <td>12.881676</td>\n",
              "      <td>11.987216</td>\n",
              "      <td>13.026751</td>\n",
              "      <td>13.153480</td>\n",
              "      <td>10.450193</td>\n",
              "      <td>13.161723</td>\n",
              "      <td>13.596879</td>\n",
              "      <td>11.434590</td>\n",
              "      <td>14.585531</td>\n",
              "      <td>13.730789</td>\n",
              "      <td>11.512010</td>\n",
              "      <td>12.484114</td>\n",
              "      <td>14.098276</td>\n",
              "      <td>14.364424</td>\n",
              "      <td>...</td>\n",
              "      <td>7.254920</td>\n",
              "      <td>11.490834</td>\n",
              "      <td>12.592521</td>\n",
              "      <td>11.105898</td>\n",
              "      <td>5.272559</td>\n",
              "      <td>5.029400</td>\n",
              "      <td>5.249350</td>\n",
              "      <td>5.519213</td>\n",
              "      <td>11.553369</td>\n",
              "      <td>10.019600</td>\n",
              "      <td>5.797745</td>\n",
              "      <td>5.189250</td>\n",
              "      <td>4.948964</td>\n",
              "      <td>4.903961</td>\n",
              "      <td>4.912838</td>\n",
              "      <td>5.035756</td>\n",
              "      <td>5.089161</td>\n",
              "      <td>4.999980</td>\n",
              "      <td>5.142136</td>\n",
              "      <td>5.410162</td>\n",
              "      <td>4.954773</td>\n",
              "      <td>5.001869</td>\n",
              "      <td>5.101809</td>\n",
              "      <td>5.365714</td>\n",
              "      <td>5.369586</td>\n",
              "      <td>9.725619</td>\n",
              "      <td>9.096012</td>\n",
              "      <td>9.199884</td>\n",
              "      <td>10.368645</td>\n",
              "      <td>10.660113</td>\n",
              "      <td>13.140723</td>\n",
              "      <td>13.189933</td>\n",
              "      <td>14.715652</td>\n",
              "      <td>14.393195</td>\n",
              "      <td>5.201713</td>\n",
              "      <td>5.175133</td>\n",
              "      <td>5.124318</td>\n",
              "      <td>4.968709</td>\n",
              "      <td>4.999055</td>\n",
              "      <td>5.006359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>925</td>\n",
              "      <td>normal</td>\n",
              "      <td>10.158716</td>\n",
              "      <td>7.034690</td>\n",
              "      <td>5.482332</td>\n",
              "      <td>7.635554</td>\n",
              "      <td>5.357286</td>\n",
              "      <td>9.562766</td>\n",
              "      <td>6.007403</td>\n",
              "      <td>5.603556</td>\n",
              "      <td>10.600154</td>\n",
              "      <td>5.420295</td>\n",
              "      <td>5.584818</td>\n",
              "      <td>9.163705</td>\n",
              "      <td>6.316064</td>\n",
              "      <td>10.801244</td>\n",
              "      <td>7.428380</td>\n",
              "      <td>8.216016</td>\n",
              "      <td>5.937365</td>\n",
              "      <td>6.262895</td>\n",
              "      <td>7.526739</td>\n",
              "      <td>6.769016</td>\n",
              "      <td>10.471970</td>\n",
              "      <td>11.398313</td>\n",
              "      <td>12.293539</td>\n",
              "      <td>14.302296</td>\n",
              "      <td>12.274100</td>\n",
              "      <td>11.884261</td>\n",
              "      <td>13.023931</td>\n",
              "      <td>13.039100</td>\n",
              "      <td>8.455113</td>\n",
              "      <td>12.793876</td>\n",
              "      <td>13.480846</td>\n",
              "      <td>10.544276</td>\n",
              "      <td>14.710931</td>\n",
              "      <td>13.535027</td>\n",
              "      <td>11.082722</td>\n",
              "      <td>12.151251</td>\n",
              "      <td>13.992824</td>\n",
              "      <td>14.221049</td>\n",
              "      <td>...</td>\n",
              "      <td>6.038053</td>\n",
              "      <td>12.160185</td>\n",
              "      <td>13.689416</td>\n",
              "      <td>12.348860</td>\n",
              "      <td>5.928123</td>\n",
              "      <td>5.235672</td>\n",
              "      <td>5.431926</td>\n",
              "      <td>5.539327</td>\n",
              "      <td>12.578100</td>\n",
              "      <td>10.896501</td>\n",
              "      <td>6.001054</td>\n",
              "      <td>5.289057</td>\n",
              "      <td>5.032878</td>\n",
              "      <td>5.000828</td>\n",
              "      <td>5.171884</td>\n",
              "      <td>5.102308</td>\n",
              "      <td>5.387260</td>\n",
              "      <td>5.134087</td>\n",
              "      <td>5.117789</td>\n",
              "      <td>5.766971</td>\n",
              "      <td>5.832299</td>\n",
              "      <td>5.114209</td>\n",
              "      <td>5.281490</td>\n",
              "      <td>5.336783</td>\n",
              "      <td>5.411891</td>\n",
              "      <td>11.663485</td>\n",
              "      <td>11.104313</td>\n",
              "      <td>11.115273</td>\n",
              "      <td>12.125272</td>\n",
              "      <td>12.460471</td>\n",
              "      <td>14.231880</td>\n",
              "      <td>14.389727</td>\n",
              "      <td>15.129897</td>\n",
              "      <td>14.976616</td>\n",
              "      <td>5.583674</td>\n",
              "      <td>5.255619</td>\n",
              "      <td>5.053729</td>\n",
              "      <td>5.146267</td>\n",
              "      <td>5.194548</td>\n",
              "      <td>5.115405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>928</td>\n",
              "      <td>normal</td>\n",
              "      <td>11.163464</td>\n",
              "      <td>7.016795</td>\n",
              "      <td>5.631753</td>\n",
              "      <td>7.709444</td>\n",
              "      <td>5.182593</td>\n",
              "      <td>9.191572</td>\n",
              "      <td>5.652939</td>\n",
              "      <td>5.704759</td>\n",
              "      <td>9.142179</td>\n",
              "      <td>5.554153</td>\n",
              "      <td>5.670742</td>\n",
              "      <td>9.560064</td>\n",
              "      <td>5.729186</td>\n",
              "      <td>10.563195</td>\n",
              "      <td>6.850626</td>\n",
              "      <td>8.562771</td>\n",
              "      <td>5.549402</td>\n",
              "      <td>6.458422</td>\n",
              "      <td>7.285570</td>\n",
              "      <td>6.848308</td>\n",
              "      <td>10.861576</td>\n",
              "      <td>11.767165</td>\n",
              "      <td>12.392304</td>\n",
              "      <td>14.343352</td>\n",
              "      <td>12.782968</td>\n",
              "      <td>11.912306</td>\n",
              "      <td>13.202460</td>\n",
              "      <td>13.122989</td>\n",
              "      <td>9.055861</td>\n",
              "      <td>13.143364</td>\n",
              "      <td>13.598588</td>\n",
              "      <td>11.530360</td>\n",
              "      <td>14.607736</td>\n",
              "      <td>13.634449</td>\n",
              "      <td>11.419988</td>\n",
              "      <td>12.453095</td>\n",
              "      <td>14.023034</td>\n",
              "      <td>14.334622</td>\n",
              "      <td>...</td>\n",
              "      <td>6.503104</td>\n",
              "      <td>12.272960</td>\n",
              "      <td>12.712224</td>\n",
              "      <td>11.664569</td>\n",
              "      <td>5.367998</td>\n",
              "      <td>5.241049</td>\n",
              "      <td>5.410512</td>\n",
              "      <td>5.483013</td>\n",
              "      <td>12.492002</td>\n",
              "      <td>10.787740</td>\n",
              "      <td>5.848194</td>\n",
              "      <td>5.342132</td>\n",
              "      <td>4.974807</td>\n",
              "      <td>4.913624</td>\n",
              "      <td>4.979323</td>\n",
              "      <td>5.033628</td>\n",
              "      <td>5.215424</td>\n",
              "      <td>5.013514</td>\n",
              "      <td>5.100513</td>\n",
              "      <td>5.363077</td>\n",
              "      <td>5.253550</td>\n",
              "      <td>5.125690</td>\n",
              "      <td>5.172440</td>\n",
              "      <td>5.349054</td>\n",
              "      <td>5.183546</td>\n",
              "      <td>9.300209</td>\n",
              "      <td>8.884612</td>\n",
              "      <td>8.812066</td>\n",
              "      <td>10.118115</td>\n",
              "      <td>10.345607</td>\n",
              "      <td>12.832546</td>\n",
              "      <td>12.921608</td>\n",
              "      <td>14.660357</td>\n",
              "      <td>14.301705</td>\n",
              "      <td>5.360712</td>\n",
              "      <td>5.078978</td>\n",
              "      <td>5.115057</td>\n",
              "      <td>4.921319</td>\n",
              "      <td>5.032157</td>\n",
              "      <td>5.052324</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows  22279 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   samples    type  ...  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at\n",
              "0      909  normal  ...         5.012729         5.032029\n",
              "1      911  normal  ...         5.156410         5.270410\n",
              "2      913  normal  ...         5.170374         5.031002\n",
              "3      916  normal  ...         5.248072         5.063287\n",
              "4      917  normal  ...         5.236353         5.356429\n",
              "5      920  normal  ...         5.068430         4.949972\n",
              "6      922  normal  ...         4.970727         4.974458\n",
              "7      924  normal  ...         4.999055         5.006359\n",
              "8      925  normal  ...         5.194548         5.115405\n",
              "9      928  normal  ...         5.032157         5.052324\n",
              "\n",
              "[10 rows x 22279 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLdq8ltbl78z"
      },
      "source": [
        "#replace categorical labels with 0 (for normal) and 1 (for tumoral)\n",
        "entire_ds[\"type\"] = entire_ds[\"type\"].replace(\"tumoral\",1)\n",
        "entire_ds[\"type\"] = entire_ds[\"type\"].replace(\"normal\",0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZSXVwm_xv3s"
      },
      "source": [
        "#save the cols \"samples\" and \"type\"\n",
        "samples_col = entire_ds[\"samples\"]\n",
        "type_col = entire_ds[\"type\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTy994Qjq1wm"
      },
      "source": [
        "#let's plot the distribution of the genes' expressions\n",
        "data = entire_ds.drop(columns=[\"samples\",\"type\"])\n",
        "#data.plot(kind=\"kde\",legend=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMhtq98dwESu"
      },
      "source": [
        "#standardize the distribution of data\n",
        "scaler = preprocessing.StandardScaler()\n",
        "data = pd.DataFrame(scaler.fit_transform(data),columns=data.columns)\n",
        "#data.plot(kind=\"kde\",legend=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GrMCxmtxZrK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "e38b8a75-87a6-4692-b347-1480367bcdbd"
      },
      "source": [
        "#merge the type and samples cols again with the standardized dataset (data)\n",
        "lab = pd.DataFrame(type_col)\n",
        "sample = pd.DataFrame(samples_col)\n",
        "stand_dataset = pd.concat([sample,lab,data],axis=1)\n",
        "stand_dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>samples</th>\n",
              "      <th>type</th>\n",
              "      <th>1007_s_at</th>\n",
              "      <th>1053_at</th>\n",
              "      <th>117_at</th>\n",
              "      <th>121_at</th>\n",
              "      <th>1255_g_at</th>\n",
              "      <th>1294_at</th>\n",
              "      <th>1316_at</th>\n",
              "      <th>1320_at</th>\n",
              "      <th>1405_i_at</th>\n",
              "      <th>1431_at</th>\n",
              "      <th>1438_at</th>\n",
              "      <th>1487_at</th>\n",
              "      <th>1494_f_at</th>\n",
              "      <th>1598_g_at</th>\n",
              "      <th>160020_at</th>\n",
              "      <th>1729_at</th>\n",
              "      <th>1773_at</th>\n",
              "      <th>177_at</th>\n",
              "      <th>179_at</th>\n",
              "      <th>1861_at</th>\n",
              "      <th>200000_s_at</th>\n",
              "      <th>200001_at</th>\n",
              "      <th>200002_at</th>\n",
              "      <th>200003_s_at</th>\n",
              "      <th>200004_at</th>\n",
              "      <th>200005_at</th>\n",
              "      <th>200006_at</th>\n",
              "      <th>200007_at</th>\n",
              "      <th>200008_s_at</th>\n",
              "      <th>200009_at</th>\n",
              "      <th>200010_at</th>\n",
              "      <th>200011_s_at</th>\n",
              "      <th>200012_x_at</th>\n",
              "      <th>200013_at</th>\n",
              "      <th>200014_s_at</th>\n",
              "      <th>200015_s_at</th>\n",
              "      <th>200016_x_at</th>\n",
              "      <th>200017_at</th>\n",
              "      <th>...</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_MB_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_3_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_5_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_M_at</th>\n",
              "      <th>AFFX-LysX-3_at</th>\n",
              "      <th>AFFX-LysX-5_at</th>\n",
              "      <th>AFFX-LysX-M_at</th>\n",
              "      <th>AFFX-M27830_3_at</th>\n",
              "      <th>AFFX-M27830_5_at</th>\n",
              "      <th>AFFX-M27830_M_at</th>\n",
              "      <th>AFFX-PheX-3_at</th>\n",
              "      <th>AFFX-PheX-5_at</th>\n",
              "      <th>AFFX-PheX-M_at</th>\n",
              "      <th>AFFX-r2-Bs-dap-3_at</th>\n",
              "      <th>AFFX-r2-Bs-dap-5_at</th>\n",
              "      <th>AFFX-r2-Bs-dap-M_at</th>\n",
              "      <th>AFFX-r2-Bs-lys-3_at</th>\n",
              "      <th>AFFX-r2-Bs-lys-5_at</th>\n",
              "      <th>AFFX-r2-Bs-lys-M_at</th>\n",
              "      <th>AFFX-r2-Bs-phe-3_at</th>\n",
              "      <th>AFFX-r2-Bs-phe-5_at</th>\n",
              "      <th>AFFX-r2-Bs-phe-M_at</th>\n",
              "      <th>AFFX-r2-Bs-thr-3_s_at</th>\n",
              "      <th>AFFX-r2-Bs-thr-5_s_at</th>\n",
              "      <th>AFFX-r2-Bs-thr-M_s_at</th>\n",
              "      <th>AFFX-r2-Ec-bioB-3_at</th>\n",
              "      <th>AFFX-r2-Ec-bioB-5_at</th>\n",
              "      <th>AFFX-r2-Ec-bioB-M_at</th>\n",
              "      <th>AFFX-r2-Ec-bioC-3_at</th>\n",
              "      <th>AFFX-r2-Ec-bioC-5_at</th>\n",
              "      <th>AFFX-r2-Ec-bioD-3_at</th>\n",
              "      <th>AFFX-r2-Ec-bioD-5_at</th>\n",
              "      <th>AFFX-r2-P1-cre-3_at</th>\n",
              "      <th>AFFX-r2-P1-cre-5_at</th>\n",
              "      <th>AFFX-ThrX-3_at</th>\n",
              "      <th>AFFX-ThrX-5_at</th>\n",
              "      <th>AFFX-ThrX-M_at</th>\n",
              "      <th>AFFX-TrpnX-3_at</th>\n",
              "      <th>AFFX-TrpnX-5_at</th>\n",
              "      <th>AFFX-TrpnX-M_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>909</td>\n",
              "      <td>0</td>\n",
              "      <td>1.179505</td>\n",
              "      <td>-1.071123</td>\n",
              "      <td>-0.689799</td>\n",
              "      <td>-0.201807</td>\n",
              "      <td>-0.550619</td>\n",
              "      <td>1.261573</td>\n",
              "      <td>-0.557836</td>\n",
              "      <td>0.987395</td>\n",
              "      <td>-0.251630</td>\n",
              "      <td>-0.448654</td>\n",
              "      <td>-0.423422</td>\n",
              "      <td>0.779346</td>\n",
              "      <td>-0.236636</td>\n",
              "      <td>0.273873</td>\n",
              "      <td>-0.481050</td>\n",
              "      <td>0.392672</td>\n",
              "      <td>-0.182818</td>\n",
              "      <td>1.528535</td>\n",
              "      <td>-0.263488</td>\n",
              "      <td>1.071036</td>\n",
              "      <td>0.237507</td>\n",
              "      <td>0.391959</td>\n",
              "      <td>-0.912532</td>\n",
              "      <td>-0.087001</td>\n",
              "      <td>1.102235</td>\n",
              "      <td>-0.146565</td>\n",
              "      <td>-0.277405</td>\n",
              "      <td>0.517323</td>\n",
              "      <td>0.572566</td>\n",
              "      <td>0.889964</td>\n",
              "      <td>0.038363</td>\n",
              "      <td>0.876542</td>\n",
              "      <td>-1.199022</td>\n",
              "      <td>-0.270913</td>\n",
              "      <td>-0.526010</td>\n",
              "      <td>-0.007375</td>\n",
              "      <td>-1.778782</td>\n",
              "      <td>-0.281974</td>\n",
              "      <td>...</td>\n",
              "      <td>0.335993</td>\n",
              "      <td>0.910172</td>\n",
              "      <td>0.726863</td>\n",
              "      <td>0.439371</td>\n",
              "      <td>-0.567516</td>\n",
              "      <td>-0.803600</td>\n",
              "      <td>-0.613926</td>\n",
              "      <td>-0.675474</td>\n",
              "      <td>0.898242</td>\n",
              "      <td>-0.200765</td>\n",
              "      <td>-0.692660</td>\n",
              "      <td>-0.587147</td>\n",
              "      <td>-0.579955</td>\n",
              "      <td>-0.666977</td>\n",
              "      <td>-0.494394</td>\n",
              "      <td>-0.638962</td>\n",
              "      <td>-0.287775</td>\n",
              "      <td>-0.179841</td>\n",
              "      <td>-0.644304</td>\n",
              "      <td>-0.520390</td>\n",
              "      <td>-0.790429</td>\n",
              "      <td>-0.502517</td>\n",
              "      <td>-0.674803</td>\n",
              "      <td>-0.615106</td>\n",
              "      <td>-0.612561</td>\n",
              "      <td>-1.156334</td>\n",
              "      <td>-1.250666</td>\n",
              "      <td>-1.239753</td>\n",
              "      <td>-1.191231</td>\n",
              "      <td>-1.181558</td>\n",
              "      <td>-1.236757</td>\n",
              "      <td>-1.384235</td>\n",
              "      <td>-1.209523</td>\n",
              "      <td>-1.496987</td>\n",
              "      <td>-0.548397</td>\n",
              "      <td>-0.549971</td>\n",
              "      <td>-0.588422</td>\n",
              "      <td>-0.708152</td>\n",
              "      <td>-0.779673</td>\n",
              "      <td>-0.412636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>911</td>\n",
              "      <td>0</td>\n",
              "      <td>0.061814</td>\n",
              "      <td>-1.059531</td>\n",
              "      <td>-0.586830</td>\n",
              "      <td>1.456859</td>\n",
              "      <td>0.033475</td>\n",
              "      <td>0.503819</td>\n",
              "      <td>1.336078</td>\n",
              "      <td>-0.463019</td>\n",
              "      <td>-0.207974</td>\n",
              "      <td>0.013761</td>\n",
              "      <td>-0.971296</td>\n",
              "      <td>1.118277</td>\n",
              "      <td>0.641395</td>\n",
              "      <td>-0.078916</td>\n",
              "      <td>0.179939</td>\n",
              "      <td>-0.676124</td>\n",
              "      <td>-0.572852</td>\n",
              "      <td>2.285947</td>\n",
              "      <td>1.837519</td>\n",
              "      <td>-1.043856</td>\n",
              "      <td>-1.471882</td>\n",
              "      <td>-1.081876</td>\n",
              "      <td>-2.819705</td>\n",
              "      <td>-2.171712</td>\n",
              "      <td>0.027674</td>\n",
              "      <td>-1.992556</td>\n",
              "      <td>-3.829399</td>\n",
              "      <td>-0.618537</td>\n",
              "      <td>-1.270501</td>\n",
              "      <td>-1.203297</td>\n",
              "      <td>-3.001717</td>\n",
              "      <td>-0.028264</td>\n",
              "      <td>-4.014765</td>\n",
              "      <td>-3.359210</td>\n",
              "      <td>-1.854039</td>\n",
              "      <td>-0.856147</td>\n",
              "      <td>-2.667571</td>\n",
              "      <td>-2.435561</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.751214</td>\n",
              "      <td>0.453951</td>\n",
              "      <td>1.647555</td>\n",
              "      <td>1.691231</td>\n",
              "      <td>-0.312193</td>\n",
              "      <td>0.235871</td>\n",
              "      <td>-0.600806</td>\n",
              "      <td>0.430509</td>\n",
              "      <td>0.843638</td>\n",
              "      <td>2.130498</td>\n",
              "      <td>0.287404</td>\n",
              "      <td>-0.443827</td>\n",
              "      <td>-0.271844</td>\n",
              "      <td>-0.640163</td>\n",
              "      <td>-0.423494</td>\n",
              "      <td>-0.525965</td>\n",
              "      <td>0.027606</td>\n",
              "      <td>-0.138884</td>\n",
              "      <td>-0.367287</td>\n",
              "      <td>-0.227139</td>\n",
              "      <td>1.470494</td>\n",
              "      <td>-0.636537</td>\n",
              "      <td>-0.596707</td>\n",
              "      <td>-0.505331</td>\n",
              "      <td>-0.539163</td>\n",
              "      <td>1.038967</td>\n",
              "      <td>1.094885</td>\n",
              "      <td>1.076155</td>\n",
              "      <td>1.067635</td>\n",
              "      <td>1.086288</td>\n",
              "      <td>1.190251</td>\n",
              "      <td>1.377466</td>\n",
              "      <td>1.391268</td>\n",
              "      <td>1.265896</td>\n",
              "      <td>-0.518303</td>\n",
              "      <td>-0.742907</td>\n",
              "      <td>-0.452413</td>\n",
              "      <td>0.377902</td>\n",
              "      <td>0.202331</td>\n",
              "      <td>1.406936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>913</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.989371</td>\n",
              "      <td>-1.103013</td>\n",
              "      <td>0.079531</td>\n",
              "      <td>-0.957104</td>\n",
              "      <td>-0.157519</td>\n",
              "      <td>0.527901</td>\n",
              "      <td>0.801846</td>\n",
              "      <td>0.371366</td>\n",
              "      <td>0.503178</td>\n",
              "      <td>-0.418605</td>\n",
              "      <td>-0.847442</td>\n",
              "      <td>-0.772353</td>\n",
              "      <td>-0.284031</td>\n",
              "      <td>-0.427081</td>\n",
              "      <td>-0.323026</td>\n",
              "      <td>-0.130464</td>\n",
              "      <td>-0.228005</td>\n",
              "      <td>-0.574945</td>\n",
              "      <td>-1.092119</td>\n",
              "      <td>-1.068719</td>\n",
              "      <td>0.229405</td>\n",
              "      <td>-0.896927</td>\n",
              "      <td>0.348633</td>\n",
              "      <td>0.770500</td>\n",
              "      <td>0.251451</td>\n",
              "      <td>0.290814</td>\n",
              "      <td>0.991865</td>\n",
              "      <td>0.683608</td>\n",
              "      <td>-0.731481</td>\n",
              "      <td>0.334684</td>\n",
              "      <td>0.121480</td>\n",
              "      <td>-0.744602</td>\n",
              "      <td>1.083549</td>\n",
              "      <td>0.594854</td>\n",
              "      <td>1.060266</td>\n",
              "      <td>0.767792</td>\n",
              "      <td>1.180392</td>\n",
              "      <td>1.778936</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.015225</td>\n",
              "      <td>1.389950</td>\n",
              "      <td>1.238176</td>\n",
              "      <td>1.720345</td>\n",
              "      <td>-0.452073</td>\n",
              "      <td>-0.216771</td>\n",
              "      <td>-0.437705</td>\n",
              "      <td>-0.058825</td>\n",
              "      <td>0.851499</td>\n",
              "      <td>0.545394</td>\n",
              "      <td>-0.297005</td>\n",
              "      <td>-0.281055</td>\n",
              "      <td>-0.456719</td>\n",
              "      <td>-0.606290</td>\n",
              "      <td>-0.419221</td>\n",
              "      <td>-0.577844</td>\n",
              "      <td>-0.479238</td>\n",
              "      <td>-0.796770</td>\n",
              "      <td>-0.566558</td>\n",
              "      <td>-0.653283</td>\n",
              "      <td>-0.140223</td>\n",
              "      <td>-0.296526</td>\n",
              "      <td>-0.451127</td>\n",
              "      <td>-0.153356</td>\n",
              "      <td>-0.685385</td>\n",
              "      <td>-0.066524</td>\n",
              "      <td>0.018949</td>\n",
              "      <td>-0.033893</td>\n",
              "      <td>0.019570</td>\n",
              "      <td>0.027841</td>\n",
              "      <td>0.082320</td>\n",
              "      <td>0.283160</td>\n",
              "      <td>0.560421</td>\n",
              "      <td>0.350529</td>\n",
              "      <td>-0.610454</td>\n",
              "      <td>-0.381770</td>\n",
              "      <td>-0.437525</td>\n",
              "      <td>-0.212382</td>\n",
              "      <td>0.297764</td>\n",
              "      <td>-0.420473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>916</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.632976</td>\n",
              "      <td>-0.468287</td>\n",
              "      <td>-0.566334</td>\n",
              "      <td>0.984043</td>\n",
              "      <td>0.287189</td>\n",
              "      <td>1.897444</td>\n",
              "      <td>1.400152</td>\n",
              "      <td>-0.017594</td>\n",
              "      <td>-0.206037</td>\n",
              "      <td>0.203992</td>\n",
              "      <td>-0.445644</td>\n",
              "      <td>-0.491909</td>\n",
              "      <td>0.407620</td>\n",
              "      <td>-0.189067</td>\n",
              "      <td>-0.233575</td>\n",
              "      <td>0.244731</td>\n",
              "      <td>1.054694</td>\n",
              "      <td>-1.186786</td>\n",
              "      <td>0.531246</td>\n",
              "      <td>-0.867288</td>\n",
              "      <td>0.458753</td>\n",
              "      <td>-0.441328</td>\n",
              "      <td>0.062572</td>\n",
              "      <td>0.751635</td>\n",
              "      <td>-1.417898</td>\n",
              "      <td>0.969413</td>\n",
              "      <td>-0.154005</td>\n",
              "      <td>-1.016233</td>\n",
              "      <td>-0.585108</td>\n",
              "      <td>0.324721</td>\n",
              "      <td>0.446003</td>\n",
              "      <td>-0.256394</td>\n",
              "      <td>0.989052</td>\n",
              "      <td>-0.135710</td>\n",
              "      <td>-0.073544</td>\n",
              "      <td>-0.398364</td>\n",
              "      <td>0.646852</td>\n",
              "      <td>0.035388</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.052410</td>\n",
              "      <td>1.438667</td>\n",
              "      <td>1.705184</td>\n",
              "      <td>2.450179</td>\n",
              "      <td>-0.156731</td>\n",
              "      <td>-0.389350</td>\n",
              "      <td>-0.226317</td>\n",
              "      <td>1.410044</td>\n",
              "      <td>2.462007</td>\n",
              "      <td>2.142467</td>\n",
              "      <td>-0.480336</td>\n",
              "      <td>-0.121906</td>\n",
              "      <td>-0.318059</td>\n",
              "      <td>-0.548145</td>\n",
              "      <td>-0.416158</td>\n",
              "      <td>-0.528908</td>\n",
              "      <td>-0.149570</td>\n",
              "      <td>-0.230375</td>\n",
              "      <td>-0.591074</td>\n",
              "      <td>0.240613</td>\n",
              "      <td>1.444709</td>\n",
              "      <td>-0.407471</td>\n",
              "      <td>-0.558606</td>\n",
              "      <td>0.089337</td>\n",
              "      <td>-0.347960</td>\n",
              "      <td>1.021801</td>\n",
              "      <td>1.095808</td>\n",
              "      <td>1.028724</td>\n",
              "      <td>1.137408</td>\n",
              "      <td>1.165481</td>\n",
              "      <td>1.063032</td>\n",
              "      <td>1.142593</td>\n",
              "      <td>0.960311</td>\n",
              "      <td>1.019800</td>\n",
              "      <td>-0.509811</td>\n",
              "      <td>-0.093510</td>\n",
              "      <td>-0.241991</td>\n",
              "      <td>2.694881</td>\n",
              "      <td>0.828804</td>\n",
              "      <td>-0.174038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>917</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.548990</td>\n",
              "      <td>-1.786054</td>\n",
              "      <td>-0.588500</td>\n",
              "      <td>1.486917</td>\n",
              "      <td>1.803855</td>\n",
              "      <td>1.358112</td>\n",
              "      <td>2.455957</td>\n",
              "      <td>0.071859</td>\n",
              "      <td>0.812621</td>\n",
              "      <td>0.016695</td>\n",
              "      <td>-0.251541</td>\n",
              "      <td>-0.571724</td>\n",
              "      <td>0.499082</td>\n",
              "      <td>-0.128979</td>\n",
              "      <td>0.879358</td>\n",
              "      <td>0.253962</td>\n",
              "      <td>0.847250</td>\n",
              "      <td>0.424197</td>\n",
              "      <td>1.194026</td>\n",
              "      <td>-1.379459</td>\n",
              "      <td>-2.900129</td>\n",
              "      <td>-2.501646</td>\n",
              "      <td>-2.321185</td>\n",
              "      <td>-2.690973</td>\n",
              "      <td>-2.482516</td>\n",
              "      <td>-2.358762</td>\n",
              "      <td>-1.781641</td>\n",
              "      <td>-0.918218</td>\n",
              "      <td>-1.173462</td>\n",
              "      <td>-3.489876</td>\n",
              "      <td>-2.368793</td>\n",
              "      <td>-1.701432</td>\n",
              "      <td>0.410168</td>\n",
              "      <td>-2.403660</td>\n",
              "      <td>-1.012278</td>\n",
              "      <td>1.341193</td>\n",
              "      <td>1.147697</td>\n",
              "      <td>0.574869</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.605462</td>\n",
              "      <td>-1.607389</td>\n",
              "      <td>-1.273339</td>\n",
              "      <td>-0.263222</td>\n",
              "      <td>0.194636</td>\n",
              "      <td>0.212411</td>\n",
              "      <td>-0.218988</td>\n",
              "      <td>0.976658</td>\n",
              "      <td>-1.251975</td>\n",
              "      <td>-0.216474</td>\n",
              "      <td>-0.412398</td>\n",
              "      <td>0.288323</td>\n",
              "      <td>0.105565</td>\n",
              "      <td>-0.500693</td>\n",
              "      <td>-0.220207</td>\n",
              "      <td>-0.354737</td>\n",
              "      <td>0.753536</td>\n",
              "      <td>0.235660</td>\n",
              "      <td>-0.374652</td>\n",
              "      <td>0.104825</td>\n",
              "      <td>0.568072</td>\n",
              "      <td>-0.376041</td>\n",
              "      <td>-0.491951</td>\n",
              "      <td>-0.160120</td>\n",
              "      <td>-0.082454</td>\n",
              "      <td>1.185632</td>\n",
              "      <td>1.134070</td>\n",
              "      <td>1.008268</td>\n",
              "      <td>1.081258</td>\n",
              "      <td>1.087580</td>\n",
              "      <td>1.138478</td>\n",
              "      <td>1.162690</td>\n",
              "      <td>0.871978</td>\n",
              "      <td>0.966373</td>\n",
              "      <td>-0.320677</td>\n",
              "      <td>0.703368</td>\n",
              "      <td>-0.327950</td>\n",
              "      <td>3.086623</td>\n",
              "      <td>0.748710</td>\n",
              "      <td>2.063522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  22279 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   samples  type  1007_s_at  ...  AFFX-TrpnX-3_at  AFFX-TrpnX-5_at  AFFX-TrpnX-M_at\n",
              "0      909     0   1.179505  ...        -0.708152        -0.779673        -0.412636\n",
              "1      911     0   0.061814  ...         0.377902         0.202331         1.406936\n",
              "2      913     0  -0.989371  ...        -0.212382         0.297764        -0.420473\n",
              "3      916     0  -0.632976  ...         2.694881         0.828804        -0.174038\n",
              "4      917     0  -0.548990  ...         3.086623         0.748710         2.063522\n",
              "\n",
              "[5 rows x 22279 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FectBldrM73",
        "outputId": "61f1fa5c-225a-40f3-8881-df39e522c199"
      },
      "source": [
        "data.shape\n",
        "entire_ds.shape\n",
        "#data[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 22279)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fecVrOzjsy2j"
      },
      "source": [
        "## *Step 3 - PCA*\n",
        "Looking at the variability present in my dataset is a good practise before starting the machine learning analysis. In particular, using PCA could be worth and give some more information about the data variability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcCQDAD403sX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "dca3d591-4fcf-409c-80cc-5b7a7897221f"
      },
      "source": [
        "#study the dataset variability with PCA\n",
        "pca = sklearnPCA(n_components=3)\n",
        "X_reduced  = pca.fit_transform(data)\n",
        "Y=lab\n",
        "plt.clf()\n",
        "fig = plt.figure(1, figsize=(10,6 ))\n",
        "ax = Axes3D(fig, elev=48, azim=134,)\n",
        "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.coolwarm,linewidths=10)\n",
        "ax.set_title(\"3D dimensionality reduction\")\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.w_zaxis.set_ticklabels([])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbjtZvDfuU7I"
      },
      "source": [
        "PCA doesn't seem to find 3 main PCs that are able to separate the two classes of data (Tumoral vs Normal). For this reason, I have to check how many components are necessary to explain at least the 95% of variability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSFNSk1A2Ozm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc769c8-9d12-497d-e5fa-7b336101c277"
      },
      "source": [
        "#find the number of components necessary to explain 95% of data variability\n",
        "pcaComponents = sklearnPCA(n_components=0.95)\n",
        "reduced  = pcaComponents.fit_transform(data)\n",
        "print(reduced.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105, 73)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzfEojM3v2IT"
      },
      "source": [
        "The number of components necessary to explain 95% of the variability in the dataset is 73."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJqJftjl7RHB"
      },
      "source": [
        "d = {0:0}\n",
        "for i in range(5,100,5):\n",
        "  pcaComponents = sklearnPCA(n_components=i/100)\n",
        "  reduced = pcaComponents.fit_transform(data)\n",
        "  d[reduced.shape[1]]=i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eaRMFWKAkYd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "39dbde4a-9a6a-4a2e-db56-466e70d5f71e"
      },
      "source": [
        "lists = d.items()\n",
        "x_axis,y_axis = zip(*lists)\n",
        "plt.plot(x_axis,y_axis,\"-o\")\n",
        "plt.ylabel(\"Cumulative explained variance\")\n",
        "plt.xlabel(\"Number of components\")\n",
        "x0 = [73]\n",
        "y0 = [95]\n",
        "plt.plot(x0,y0,\"s\")\n",
        "plt.ylim((0,100))\n",
        "plt.xlim((0,80))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 80.0)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5GsTUg-wCij"
      },
      "source": [
        "# **Solving the Classification problem**\n",
        "Firstly I have to split my dataset into a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IouiKDV3wPKQ"
      },
      "source": [
        "#split the dataset\n",
        "train,test = train_test_split(stand_dataset,test_size=0.2,random_state=10)\n",
        "train_y = train.type\n",
        "train_x = train.drop(columns= [\"type\",\"samples\"])\n",
        "test_y = test.type\n",
        "test_x = test.drop(columns= [\"type\",\"samples\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PTFVnh8y_nC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea814f1-aa1a-46c0-a0f5-8b7de3bbe120"
      },
      "source": [
        "print(train[\"type\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    44\n",
            "0    40\n",
            "Name: type, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B44nNClrz2FK"
      },
      "source": [
        "The classes in the training set are slightly unbalanced. For this reason, I use the SMOTE library, which uses a nearest neighbors algorithm to generate new and synthetic data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij0igx-D1i9Y"
      },
      "source": [
        "oversample = SMOTE(random_state=12)\n",
        "train_x,train_y = oversample.fit_resample(train_x,train_y)\n",
        "test_x,test_y = oversample.fit_resample(test_x,test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atHBba6h4KPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcca2f7-8f91-4a72-bca4-61cbdbc893d9"
      },
      "source": [
        "test_y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    13\n",
              "0    13\n",
              "Name: type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa0xg2P772cN"
      },
      "source": [
        "## *Classifier 1 - Logistic Regression*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ937TRQ8Ezn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f905aa8c-4f53-4339-c408-8cd5c497dd2d"
      },
      "source": [
        "folds = [2,4,6,8,10]\n",
        "seed = 7\n",
        "model = LogisticRegression()\n",
        "results = []\n",
        "scoring_list = [\"accuracy\",\"roc_auc\"]\n",
        "for fold in folds:\n",
        "  print(\"\\nResults of %s-fold cross-validation\"%fold)\n",
        "  kfold = KFold(n_splits=fold, random_state=seed,shuffle=True)\n",
        "  for scoring in scoring_list:\n",
        "    cv_results = cross_val_score(model, train_x, train_y.ravel(), cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    print(scoring+\": \"+str(round(cv_results.mean(),3))+\" +/- \"+str(round(cv_results.std(),3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results of 2-fold cross-validation\n",
            "accuracy: 0.875 +/- 0.034\n",
            "roc_auc: 0.857 +/- 0.039\n",
            "\n",
            "Results of 4-fold cross-validation\n",
            "accuracy: 0.852 +/- 0.059\n",
            "roc_auc: 0.879 +/- 0.053\n",
            "\n",
            "Results of 6-fold cross-validation\n",
            "accuracy: 0.842 +/- 0.082\n",
            "roc_auc: 0.865 +/- 0.1\n",
            "\n",
            "Results of 8-fold cross-validation\n",
            "accuracy: 0.852 +/- 0.063\n",
            "roc_auc: 0.891 +/- 0.096\n",
            "\n",
            "Results of 10-fold cross-validation\n",
            "accuracy: 0.854 +/- 0.1\n",
            "roc_auc: 0.871 +/- 0.152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKns1D1YJB33"
      },
      "source": [
        "Logistic regression seems to perform quite good for both accuracy and roc_auc. However, I can perform the tuning of the algorithm, also called hyperparameter optimization. One approach to perform this task is the Grid Search approach, which will build and evaluate a model for each combination of hyperparameters specified in a grid. The dual is set to True since the number of features >> number of examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DMcTGT2Js7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f9a566-60a3-4499-ef19-8e0cb37a11a3"
      },
      "source": [
        "#Grid Search\n",
        "hyperpar = {\"C\":[1e-10, 1e-8, 1e-6, 1e-4,1e-2,1e-1,1,10,100]}\n",
        "grid = GridSearchCV(estimator=LogisticRegression(dual=True,solver=\"liblinear\",random_state = 7),param_grid=hyperpar,scoring=\"accuracy\")\n",
        "grid.fit(train_x,train_y.ravel())\n",
        "best_accuracy = grid.best_score_\n",
        "best_hyperp = grid.best_params_\n",
        "print(\"Best CV accuracy: \",best_accuracy)\n",
        "print(\"Best Params: \",best_hyperp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best CV accuracy:  0.8633986928104577\n",
            "Best Params:  {'C': 1e-10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xM8R7MhNb41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e0b3ce-d37b-4052-f645-41dcbb1a4189"
      },
      "source": [
        "#Make predictions on the test set\n",
        "predictions = grid.predict(test_x)\n",
        "print(\"Accuracy on Test Set: \",sklearn.metrics.accuracy_score(test_y.ravel(), predictions))\n",
        "print(\"AUC: \",sklearn.metrics.roc_auc_score(test_y.ravel(), predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test Set:  0.8461538461538461\n",
            "AUC:  0.8461538461538461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKdhhwq_Q_j4"
      },
      "source": [
        "Even tuning the hyperparameter C, no great improvement has been reached with the Grid Search approach. The reason can be due to the fact that there are no features (since the PCA failed in separating data in 3D space) that can explain an high percentage of variability, meaning that there is no feature associated with high weight. Thus, a simple linear logistic regression may not be able to correctly make the classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61S3Py86TR8K"
      },
      "source": [
        "## *Classifier 2 - Naive Bayes (NB)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuyOlRImTapJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33932a14-7dae-49e4-bb09-c0ecfff506b2"
      },
      "source": [
        "#Perform the cross-validation procedure\n",
        "folds = [2,4,6,8,10]\n",
        "seed = 7\n",
        "model = GaussianNB()\n",
        "results = []\n",
        "scoring_list = [\"accuracy\",\"roc_auc\"]\n",
        "for fold in folds:\n",
        "  print(\"\\nResults of %s-fold cross-validation\"%fold)\n",
        "  kfold = KFold(n_splits=fold, random_state=seed,shuffle=True)\n",
        "  for scoring in scoring_list:\n",
        "    cv_results = cross_val_score(model, train_x, train_y.ravel(), cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    print(scoring+\": \"+str(round(cv_results.mean(),3))+\" +/- \"+str(round(cv_results.std(),3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results of 2-fold cross-validation\n",
            "accuracy: 0.852 +/- 0.034\n",
            "roc_auc: 0.847 +/- 0.03\n",
            "\n",
            "Results of 4-fold cross-validation\n",
            "accuracy: 0.864 +/- 0.079\n",
            "roc_auc: 0.877 +/- 0.083\n",
            "\n",
            "Results of 6-fold cross-validation\n",
            "accuracy: 0.818 +/- 0.121\n",
            "roc_auc: 0.848 +/- 0.119\n",
            "\n",
            "Results of 8-fold cross-validation\n",
            "accuracy: 0.83 +/- 0.084\n",
            "roc_auc: 0.851 +/- 0.102\n",
            "\n",
            "Results of 10-fold cross-validation\n",
            "accuracy: 0.854 +/- 0.111\n",
            "roc_auc: 0.867 +/- 0.112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3ahZNfEUbj_"
      },
      "source": [
        "On the cross validation procedure, the NB performs similar to Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4dPWQTfU2V9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10226ded-3d8e-4b49-b250-d2ca1947602b"
      },
      "source": [
        "#Perform the prediction on the test set\n",
        "model = GaussianNB()\n",
        "model.fit(train_x,train_y.ravel())\n",
        "predictions = model.predict(test_x)\n",
        "print(\"Accuracy on Test Set: \",sklearn.metrics.accuracy_score(test_y.ravel(), predictions))\n",
        "print(\"AUC: \",sklearn.metrics.roc_auc_score(test_y.ravel(), predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test Set:  0.8461538461538461\n",
            "AUC:  0.8461538461538461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fewvl-3iVkkw"
      },
      "source": [
        "The accuracy on the Test Set using NB is more or less the same of Linear Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTey7K62WB_W"
      },
      "source": [
        "## *Classifier 3 - Decision Trees*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CylZlVxQ0rGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805c728d-c3d6-4cc5-8d86-71cfaf74805f"
      },
      "source": [
        "#train and fine-tune the decision tree using the GridSearchCV class\n",
        "params = {'max_leaf_nodes': list(range(2,18,2)), 'min_samples_split': [2, 3, 4],\"max_depth\":list(range(2,10,2))}\n",
        "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=7), params)\n",
        "grid_search_cv.fit(train_x, train_y.ravel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=DecisionTreeClassifier(random_state=7),\n",
              "             param_grid={'max_depth': [2, 4, 6, 8],\n",
              "                         'max_leaf_nodes': [2, 4, 6, 8, 10, 12, 14, 16],\n",
              "                         'min_samples_split': [2, 3, 4]})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5leJ0ZT5A3_i",
        "outputId": "b723bee5-2593-4127-ce60-6058aecdb105"
      },
      "source": [
        "#get best accuracy and params\n",
        "best_accuracy = grid_search_cv.best_score_ \n",
        "best_params = grid_search_cv.best_params_ \n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best CV accuracy: \", best_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_split': 2}\n",
            "Best CV accuracy:  0.8071895424836601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X44R27SS3_xi",
        "outputId": "21f42eb6-4295-4826-b534-3d9cb9adcb6e"
      },
      "source": [
        "#make the prediction on the test set\n",
        "prediction = grid_search_cv.predict(test_x)\n",
        "print(\"Accuracy Test: \", sklearn.metrics.accuracy_score(test_y, prediction))\n",
        "print(\"AUC Test: \", sklearn.metrics.roc_auc_score(test_y, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Test:  0.7307692307692307\n",
            "AUC Test:  0.7307692307692307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwgNTtI86VtU"
      },
      "source": [
        "For the purity score, the default Gini index is kept. The accuracy in this case is lower than the previous accuracies obtained with other classifiers. Moreover, with DT it seems that overfitting is affecting the training procedure, since the difference in accuracy between train and test set evident."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVVw7xcL6xaj"
      },
      "source": [
        "## *Classifier 4 - Random Forest*\n",
        "This is an Ensemble learner. A random forest is composed of multiple decision trees and usually it reaches an higher accuracy with respect to single decision trees. \n",
        "The parameters to choose are the same, with the n_estimators hyperparameter which is added in the Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wDueXO862Wv",
        "outputId": "a80fcfd4-7e58-4d7d-f506-d6e135d75277"
      },
      "source": [
        "params = {'n_estimators': [10,20,30,40,50],'max_leaf_nodes': list(range(2,8,2)), 'min_samples_split': [2, 3, 4],\"max_depth\":list(range(2,6,2))}\n",
        "model = GridSearchCV(estimator=RandomForestClassifier(random_state=7), param_grid=params)\n",
        "model.fit(train_x, train_y.ravel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=RandomForestClassifier(random_state=7),\n",
              "             param_grid={'max_depth': [2, 4], 'max_leaf_nodes': [2, 4, 6],\n",
              "                         'min_samples_split': [2, 3, 4],\n",
              "                         'n_estimators': [10, 20, 30, 40, 50]})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXN3eSiJDCTX",
        "outputId": "9c4716e8-3491-46ed-b26e-19614a78721e"
      },
      "source": [
        "#get best accuracy and params\n",
        "best_accuracy = model.best_score_ \n",
        "best_params = model.best_params_ \n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best CV accuracy: \", best_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_split': 2, 'n_estimators': 20}\n",
            "Best CV accuracy:  0.8869281045751635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pav9o4yODJ29",
        "outputId": "22e86532-13ff-4fce-b10d-a7dc3685b713"
      },
      "source": [
        "#make the prediction on the test set\n",
        "prediction = model.predict(test_x)\n",
        "print(\"Accuracy Test: \", sklearn.metrics.accuracy_score(test_y.ravel(), prediction))\n",
        "print(\"AUC Test: \", sklearn.metrics.roc_auc_score(test_y.ravel(), prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Test:  0.8076923076923077\n",
            "AUC Test:  0.8076923076923077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUMbYvUGITh3"
      },
      "source": [
        "The accuracy on the training set has significantly increased with the random forest, however in the test set the accuracy is very different, reflecting the same problem of overfitting of DTs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF6fWoPmJDIQ"
      },
      "source": [
        "## *Classifier 5 - K-Nearest Neighbors*\n",
        "The main two hyperparameters in this classifier are the number of nearest neighbours and the weights associated to each neighbour which influences the classification. *Uniform* and *Distance* give respectively a equal weight to all the neighbours of a specific point and a *weighted score* inversally proportional to the distance of a neighbour to the point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd5AQ6GKJLdf",
        "outputId": "c36ff693-9a63-4fcd-83a8-bc19b7f82ea5"
      },
      "source": [
        "params = {'n_neighbors': list(range(3,30)),\"weights\": [\"uniform\", \"distance\"],\"p\":[1,2]}\n",
        "model = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=params)\n",
        "model.fit(train_x, train_y.ravel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=KNeighborsClassifier(),\n",
              "             param_grid={'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
              "                                         14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
              "                                         24, 25, 26, 27, 28, 29],\n",
              "                         'p': [1, 2], 'weights': ['uniform', 'distance']})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG-X6xd8-x5E",
        "outputId": "e11b4d9f-ec77-4417-f96b-4000f0a9d62c"
      },
      "source": [
        "#get best accuracy and params\n",
        "best_accuracy = model.best_score_\n",
        "best_params = model.best_params_ \n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best CV accuracy: \", best_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
            "Best CV accuracy:  0.7843137254901962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iABXlqs-1jt",
        "outputId": "ace18c5b-b415-466a-cd05-7c101b5f2e65"
      },
      "source": [
        "#make the prediction on the test set\n",
        "prediction = model.predict(test_x)\n",
        "print(\"Accuracy Test: \", sklearn.metrics.accuracy_score(test_y, prediction))\n",
        "print(\"AUC Test: \", sklearn.metrics.roc_auc_score(test_y, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Test:  0.7692307692307693\n",
            "AUC Test:  0.7692307692307692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuS3KvwyBCHU"
      },
      "source": [
        "As it is shown, the classifier doesn't seem to perform good, however the overfitting here is an almost neglectable phenomena."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-DT9pDnH6CA"
      },
      "source": [
        "## *Classifier 6 - Support Vector Machines (SVMs)*\n",
        "Even with the SVM, the main hyperparameter is the C regularization term. I can also use different types of Kernel functions, such as the linear, the polynomial, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4DJLfyoH5RA",
        "outputId": "aa8c579c-b1bb-4918-84af-4452a1823c87"
      },
      "source": [
        "parameter = [{'C':[1e-5,1e-4,1e-3,1e-2,1e-1,1,2,3,10], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}]\n",
        "model = GridSearchCV(estimator=SVC(random_state=7), param_grid=parameter)\n",
        "model.fit(train_x, train_y.ravel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=SVC(random_state=7),\n",
              "             param_grid=[{'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 2, 3, 10],\n",
              "                          'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mntrWsOBJR2a",
        "outputId": "b292288f-c491-4ffc-cf16-e75495071509"
      },
      "source": [
        "#get best accuracy and params\n",
        "best_accuracy = model.best_score_\n",
        "best_params = model.best_params_\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best CV accuracy: \", best_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'C': 1, 'kernel': 'rbf'}\n",
            "Best CV accuracy:  0.8862745098039216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE9SZd0zJcdO",
        "outputId": "f83e315b-d263-402f-f4cd-de60eaa130af"
      },
      "source": [
        "#make the prediction on the test set\n",
        "prediction = model.predict(test_x)\n",
        "print(\"Accuracy Test: \", sklearn.metrics.accuracy_score(test_y.ravel(), prediction))\n",
        "print(\"AUC Test: \", sklearn.metrics.roc_auc_score(test_y.ravel(), prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Test:  0.8076923076923077\n",
            "AUC Test:  0.8076923076923077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33BitfdnK_32"
      },
      "source": [
        "In case of SVMs, the accuracy in the training set increases (88%), there is a high problem of overfitting, since in the test set the accuracy decreases to 80%. Even increasing the C hyperparameter the problem persists: the only solution is to increase the number of examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UTJ783zMvKb"
      },
      "source": [
        "# **Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr_tjYn3M2aN"
      },
      "source": [
        "## *Multilayer Perceptron (MLP)*\n",
        "The simplest NN architecture is the MLP. Here, I implemented a MLP using the Sequential interface of Keras. For the optimizer I decided to use Adam, which is usually a good default choice. The network here is composed of 3 dense layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8f4SJFHPoFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2792f6-1947-4791-da15-ea7a76815240"
      },
      "source": [
        "def make_model():\n",
        "  set_seed(432)\n",
        "  nn_model = keras.models.Sequential()\n",
        "  nn_model.add(keras.layers.Dense(1400,activation = \"relu\",input_shape=(22277,)))\n",
        "  nn_model.add(keras.layers.Dense(800,activation = \"relu\"))\n",
        "  nn_model.add(keras.layers.Dense(1,activation = \"sigmoid\"))\n",
        "\n",
        "  opt = optimizers.Adam(learning_rate=0.05)\n",
        "  nn_model.compile(optimizer=opt,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "  return nn_model\n",
        "\n",
        "params={'batch_size':[15,20], \n",
        "        'epochs':[5,7]}\n",
        "\n",
        "my_classifier=KerasClassifier(make_model)\n",
        "validator_nn = GridSearchCV(my_classifier, param_grid=params,return_train_score=True)\n",
        "validator_nn.fit(train_x, train_y, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 180ms/step - loss: 19876.2402 - accuracy: 0.6143\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 12261.3623 - accuracy: 0.6000\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 3789.3911 - accuracy: 0.8143\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 1s 165ms/step - loss: 1608.9777 - accuracy: 0.8857\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 2277.6890 - accuracy: 0.8286\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 1316.8893 - accuracy: 0.9000\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 278.5972 - accuracy: 0.9714\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 54381.4297 - accuracy: 0.7778\n",
            "5/5 [==============================] - 0s 50ms/step - loss: 26.1581 - accuracy: 0.9857\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 15955.4072 - accuracy: 0.5857\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 7664.7266 - accuracy: 0.8000\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 8837.5674 - accuracy: 0.7857\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 2163.6084 - accuracy: 0.9143\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 4433.5171 - accuracy: 0.8714\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 11.9773 - accuracy: 0.9857\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 779.6114 - accuracy: 0.9714\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 7412.9663 - accuracy: 0.8889\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.9628 - accuracy: 0.9857\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 21358.8574 - accuracy: 0.6000\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 1s 160ms/step - loss: 9884.7686 - accuracy: 0.8143\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 7032.2563 - accuracy: 0.8143\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 5546.4707 - accuracy: 0.9143\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 5047.3989 - accuracy: 0.8143\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 628.3817 - accuracy: 0.9429\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 1196.5775 - accuracy: 0.9571\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 64218.5000 - accuracy: 0.6667\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 163ms/step - loss: 26356.6699 - accuracy: 0.5493\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 8171.4189 - accuracy: 0.6901\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 5825.8311 - accuracy: 0.8028\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 2312.4709 - accuracy: 0.8310\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 1s 162ms/step - loss: 307.0294 - accuracy: 0.9577\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 1s 165ms/step - loss: 729.0472 - accuracy: 0.9296\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 672.4083 - accuracy: 0.9718\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 21837.7324 - accuracy: 0.7059\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 120.6347 - accuracy: 0.9859\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 34123.1328 - accuracy: 0.5634\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 1s 164ms/step - loss: 10231.6250 - accuracy: 0.7746\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 11133.6416 - accuracy: 0.8028\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 3146.2959 - accuracy: 0.7746\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 6524.3213 - accuracy: 0.8028\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 301.0346 - accuracy: 0.9718\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 1628.0630 - accuracy: 0.8310\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2759.7017 - accuracy: 0.8824\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 81.3361 - accuracy: 0.9859\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 167ms/step - loss: 12849.8154 - accuracy: 0.5682\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 1s 164ms/step - loss: 14709.1133 - accuracy: 0.7273\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 1s 165ms/step - loss: 3652.0193 - accuracy: 0.8068\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 1s 168ms/step - loss: 2055.1201 - accuracy: 0.8750\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 8314.1963 - accuracy: 0.8750\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 3570.6929 - accuracy: 0.8523\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 1s 167ms/step - loss: 157.8272 - accuracy: 0.9545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fc0a2f28bd0>,\n",
              "             param_grid={'batch_size': [15], 'epochs': [7]},\n",
              "             return_train_score=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA2ajwOvZ7dU"
      },
      "source": [
        "res = pd.DataFrame(validator_nn.cv_results_)\n",
        "res.pivot_table(index=[\"param_epochs\",\"param_batch_size\"],values=['mean_train_score',\"mean_test_score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rVG9VxEe722",
        "outputId": "e0b85a47-c5e3-4837-e0a3-ff0694da44c8"
      },
      "source": [
        "best_accuracy = validator_nn.best_score_\n",
        "best_params = validator_nn.best_params_\n",
        "print(\"Best Parameters: \", best_params)\n",
        "print(\"Best CV accuracy: \", best_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'batch_size': 15, 'epochs': 7}\n",
            "Best CV accuracy:  0.7843137383460999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfG5-TViQN3w",
        "outputId": "2688be0d-e7da-4257-e2ff-e44b327407fe"
      },
      "source": [
        "best_model = validator_nn.best_estimator_.model\n",
        "loss,accuracy = best_model.evaluate(test_x,test_y.ravel())\n",
        "print(\"Accuracy Test: \", accuracy)\n",
        "print(\"Loss on Test: \", loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 258ms/step - loss: 1160.6202 - accuracy: 0.8462\n",
            "Accuracy Test:  0.8461538553237915\n",
            "Loss on Test:  1160.6202392578125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpvSPdPH7smP"
      },
      "source": [
        "As it is shown by the results, the NN is able to reach 98% of accuracy on the training set. However, it is evident the presence of overfitting: the accuracy on the test set drops down to 84% and also the test loss is very high. The problem of overfitting is mainly due to the low number of examples and the high number of features (that in my case are represented by the number of genes = 22277). Since I can not increase the number of real examples, what I could do is to decrease the dimensionality of the dataset: that is, to reduce the number of genes to only the genes that are able to explain 95% of the dataset variability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNABt2jE9Vf0"
      },
      "source": [
        "## *Multilayer Perceptron (MLP) after dimensionality reduction*\n",
        "The idea is to find the components that are able to explain 95% of variability and to use that subset of data to train our MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTFP6UPw9I0K",
        "outputId": "13b57865-8d50-4345-fab3-99bf00049cc8"
      },
      "source": [
        "#perform the PCA\n",
        "dim_reduction = sklearnPCA(n_components=0.95)\n",
        "trainx_reduced = dim_reduction.fit_transform(train_x)\n",
        "testx_reduced = dim_reduction.transform(test_x)\n",
        "print(trainx_reduced.shape)\n",
        "print(testx_reduced.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(88, 59)\n",
            "(26, 59)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGVnlNtb_15Q"
      },
      "source": [
        "A total number of 59 components have been found. Now I can run again the same previous model, reducing the dimension of the Dense layers. In this case, I used as optimizer Adamax, which is a more stable variation of Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syWJF0GpAL9C"
      },
      "source": [
        "def make_model(learning_rate):\n",
        "  set_seed(432)\n",
        "  nn_model_red = keras.models.Sequential()\n",
        "  nn_model_red.add(keras.layers.Dense(50, activation = \"relu\",input_shape=(59,)))\n",
        "  nn_model_red.add(keras.layers.Dense(30, activation = \"relu\"))\n",
        "  nn_model_red.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "  opt = optimizers.Adam(learning_rate=learning_rate)\n",
        "  nn_model_red.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return nn_model_red\n",
        "\n",
        "params={'batch_size':[15, 20, 25, 30, 35], \n",
        "        'epochs':[1, 3, 5, 10],\n",
        "        'learning_rate':[0.01,0.05,0.1]}\n",
        "\n",
        "my_classifier=KerasClassifier(make_model)\n",
        "validator_red_nn = GridSearchCV(my_classifier, param_grid=params,return_train_score=True)\n",
        "validator_red_nn.fit(trainx_reduced, train_y, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3nT07X4RWqy2",
        "outputId": "36ab0a65-20f2-4eab-a8b3-aff14452663f"
      },
      "source": [
        "res = pd.DataFrame(validator_red_nn.cv_results_)\n",
        "res.pivot_table(index=[\"param_epochs\",\"param_batch_size\",\"param_learning_rate\"],values=['mean_train_score',\"mean_test_score\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>param_epochs</th>\n",
              "      <th>param_batch_size</th>\n",
              "      <th>param_learning_rate</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"15\" valign=\"top\">1</th>\n",
              "      <th rowspan=\"3\" valign=\"top\">15</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.920282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.671242</td>\n",
              "      <td>0.792515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.730065</td>\n",
              "      <td>0.766881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">20</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.894608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.771895</td>\n",
              "      <td>0.832596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.603922</td>\n",
              "      <td>0.721690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">25</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.636601</td>\n",
              "      <td>0.798068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.750327</td>\n",
              "      <td>0.838068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.718833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.670588</td>\n",
              "      <td>0.812193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.750980</td>\n",
              "      <td>0.817907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.715033</td>\n",
              "      <td>0.752998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">35</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.633987</td>\n",
              "      <td>0.727646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.615033</td>\n",
              "      <td>0.695332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.522222</td>\n",
              "      <td>0.652918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"15\" valign=\"top\">3</th>\n",
              "      <th rowspan=\"3\" valign=\"top\">15</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.784967</td>\n",
              "      <td>0.977304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.750327</td>\n",
              "      <td>0.914567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.815686</td>\n",
              "      <td>0.875211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">20</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.749673</td>\n",
              "      <td>0.974326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.737255</td>\n",
              "      <td>0.926157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.818954</td>\n",
              "      <td>0.917626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">25</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.782353</td>\n",
              "      <td>0.968732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.783007</td>\n",
              "      <td>0.943139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.792113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.771242</td>\n",
              "      <td>0.951509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.816993</td>\n",
              "      <td>0.940402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.762745</td>\n",
              "      <td>0.871710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">35</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.805882</td>\n",
              "      <td>0.948853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.761438</td>\n",
              "      <td>0.917505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.682353</td>\n",
              "      <td>0.872153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"15\" valign=\"top\">5</th>\n",
              "      <th rowspan=\"3\" valign=\"top\">15</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.773203</td>\n",
              "      <td>0.997143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.852288</td>\n",
              "      <td>0.965915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.840523</td>\n",
              "      <td>0.948732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">20</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.795425</td>\n",
              "      <td>0.991469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.960282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.726144</td>\n",
              "      <td>0.923260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">25</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.816993</td>\n",
              "      <td>0.991469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.782353</td>\n",
              "      <td>0.962978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.737909</td>\n",
              "      <td>0.866237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.816340</td>\n",
              "      <td>0.985755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.739216</td>\n",
              "      <td>0.963018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.739869</td>\n",
              "      <td>0.894286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">35</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.816993</td>\n",
              "      <td>0.991469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.773203</td>\n",
              "      <td>0.931871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.579739</td>\n",
              "      <td>0.826600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"15\" valign=\"top\">10</th>\n",
              "      <th rowspan=\"3\" valign=\"top\">15</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.784314</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.841176</td>\n",
              "      <td>0.991429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.828758</td>\n",
              "      <td>0.963058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">20</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.828758</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.977304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.807190</td>\n",
              "      <td>0.968692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">25</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.795425</td>\n",
              "      <td>0.997143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.783660</td>\n",
              "      <td>0.971469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.773203</td>\n",
              "      <td>0.888773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">30</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.749673</td>\n",
              "      <td>0.997143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.794771</td>\n",
              "      <td>0.974286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.784314</td>\n",
              "      <td>0.920000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">35</th>\n",
              "      <th>0.01</th>\n",
              "      <td>0.805229</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.05</th>\n",
              "      <td>0.786275</td>\n",
              "      <td>0.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.874930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   mean_test_score  mean_train_score\n",
              "param_epochs param_batch_size param_learning_rate                                   \n",
              "1            15               0.01                        0.819608          0.920282\n",
              "                              0.05                        0.671242          0.792515\n",
              "                              0.10                        0.730065          0.766881\n",
              "             20               0.01                        0.717647          0.894608\n",
              "                              0.05                        0.771895          0.832596\n",
              "                              0.10                        0.603922          0.721690\n",
              "             25               0.01                        0.636601          0.798068\n",
              "                              0.05                        0.750327          0.838068\n",
              "                              0.10                        0.682353          0.718833\n",
              "             30               0.01                        0.670588          0.812193\n",
              "                              0.05                        0.750980          0.817907\n",
              "                              0.10                        0.715033          0.752998\n",
              "             35               0.01                        0.633987          0.727646\n",
              "                              0.05                        0.615033          0.695332\n",
              "                              0.10                        0.522222          0.652918\n",
              "3            15               0.01                        0.784967          0.977304\n",
              "                              0.05                        0.750327          0.914567\n",
              "                              0.10                        0.815686          0.875211\n",
              "             20               0.01                        0.749673          0.974326\n",
              "                              0.05                        0.737255          0.926157\n",
              "                              0.10                        0.818954          0.917626\n",
              "             25               0.01                        0.782353          0.968732\n",
              "                              0.05                        0.783007          0.943139\n",
              "                              0.10                        0.647059          0.792113\n",
              "             30               0.01                        0.771242          0.951509\n",
              "                              0.05                        0.816993          0.940402\n",
              "                              0.10                        0.762745          0.871710\n",
              "             35               0.01                        0.805882          0.948853\n",
              "                              0.05                        0.761438          0.917505\n",
              "                              0.10                        0.682353          0.872153\n",
              "5            15               0.01                        0.773203          0.997143\n",
              "                              0.05                        0.852288          0.965915\n",
              "                              0.10                        0.840523          0.948732\n",
              "             20               0.01                        0.795425          0.991469\n",
              "                              0.05                        0.772549          0.960282\n",
              "                              0.10                        0.726144          0.923260\n",
              "             25               0.01                        0.816993          0.991469\n",
              "                              0.05                        0.782353          0.962978\n",
              "                              0.10                        0.737909          0.866237\n",
              "             30               0.01                        0.816340          0.985755\n",
              "                              0.05                        0.739216          0.963018\n",
              "                              0.10                        0.739869          0.894286\n",
              "             35               0.01                        0.816993          0.991469\n",
              "                              0.05                        0.773203          0.931871\n",
              "                              0.10                        0.579739          0.826600\n",
              "10           15               0.01                        0.784314          1.000000\n",
              "                              0.05                        0.841176          0.991429\n",
              "                              0.10                        0.828758          0.963058\n",
              "             20               0.01                        0.828758          1.000000\n",
              "                              0.05                        0.784314          0.977304\n",
              "                              0.10                        0.807190          0.968692\n",
              "             25               0.01                        0.795425          0.997143\n",
              "                              0.05                        0.783660          0.971469\n",
              "                              0.10                        0.773203          0.888773\n",
              "             30               0.01                        0.749673          0.997143\n",
              "                              0.05                        0.794771          0.974286\n",
              "                              0.10                        0.784314          0.920000\n",
              "             35               0.01                        0.805229          1.000000\n",
              "                              0.05                        0.786275          0.960000\n",
              "                              0.10                        0.647059          0.874930"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlhFpVsTKrrZ",
        "outputId": "ecd4a041-9b83-4260-dbe7-1e4bd80a9c1b"
      },
      "source": [
        "print(\"Best hyperparameters: \",validator_red_nn.best_params_)\n",
        "print(\"Best CV accuracy: \",validator_red_nn.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'batch_size': 15, 'epochs': 5, 'learning_rate': 0.05}\n",
            "Best CV accuracy:  0.8522875785827637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJE4RTpKLnmm",
        "outputId": "2d62a8d1-59a2-4dca-ede0-dc4a3d30ed80"
      },
      "source": [
        "best_model = validator_red_nn.best_estimator_.model\n",
        "loss,accuracy = best_model.evaluate(testx_reduced,test_y)\n",
        "print(\"Accuracy Test: \", accuracy)\n",
        "print(\"Loss on Test: \", loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 122ms/step - loss: 0.9116 - accuracy: 0.9231\n",
            "Accuracy Test:  0.9230769276618958\n",
            "Loss on Test:  0.9116257429122925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvGd8ihyBuMz"
      },
      "source": [
        "Using PCA before running the MLP has demonstrated to help the training and reduce the overfitting: the accuracy on the training set has reached 96%, while the accuracy on the test set 92%. Thus, using a MLP with PCA has strongly increased the accuracy and decreased the problem of overfitting, even if is still present if we look at the CV results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk1CbOLS1Lip"
      },
      "source": [
        "## *MLP with PCA and Dropout*\n",
        "Dropout is a techinque used to prevent overfitting and it is the most popular regularization technique for DNNs. In few words, during the training procedure, at each iteration, a neuron is temporarily disabled with a probability p. This probability is an hyperparameter and is called dropout-rate. In my case I used a dropout-rate equal to 0.2 since I am not actually dealing with a DNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1DZ6g-O1Vjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa120be-2281-4dcc-992f-433be7fb9236"
      },
      "source": [
        "def make_model():\n",
        "  set_seed(432)\n",
        "  nn_model_red = keras.models.Sequential()\n",
        "  nn_model_red.add(keras.layers.Dense(50, activation = \"relu\",input_shape=(59,)))\n",
        "  nn_model_red.add(keras.layers.Dropout(0.2))\n",
        "  nn_model_red.add(keras.layers.Dense(30, activation = \"relu\"))\n",
        "  nn_model_red.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "  opt = optimizers.Adam(learning_rate=0.05)\n",
        "  nn_model_red.compile(optimizer=opt,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return nn_model_red\n",
        "params={'batch_size':[15, 20, 25], \n",
        "        'epochs':[3, 5,7]}\n",
        "\n",
        "my_classifier=KerasClassifier(make_model)\n",
        "validator_red_drop_nn = GridSearchCV(my_classifier, param_grid=params,return_train_score=True)\n",
        "validator_red_drop_nn.fit(trainx_reduced, train_y, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 10.7099 - accuracy: 0.5714\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.6205 - accuracy: 0.8429\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.9000\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.9545 - accuracy: 0.8333\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2386 - accuracy: 0.8714\n",
            "Epoch 1/3\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 10.0720 - accuracy: 0.5714\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.9340 - accuracy: 0.7143\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.8903 - accuracy: 0.8857\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 2.5052 - accuracy: 0.8333\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.5790 - accuracy: 0.9429\n",
            "Epoch 1/3\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 8.2232 - accuracy: 0.6143\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.4663 - accuracy: 0.8000\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2571 - accuracy: 0.9714\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 14.1809 - accuracy: 0.7778\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.9429\n",
            "Epoch 1/3\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 11.0894 - accuracy: 0.6197\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.6310 - accuracy: 0.8028\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.4534 - accuracy: 0.7606\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.9199 - accuracy: 0.7647\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3412 - accuracy: 0.8451\n",
            "Epoch 1/3\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 11.5195 - accuracy: 0.6197\n",
            "Epoch 2/3\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.3167 - accuracy: 0.6901\n",
            "Epoch 3/3\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.4651 - accuracy: 0.8873\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.8708 - accuracy: 0.8824\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9718\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 10.7099 - accuracy: 0.5714\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.6205 - accuracy: 0.8429\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.9000\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8857\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9429\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7444 - accuracy: 0.8333\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9571\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 10.0720 - accuracy: 0.5714\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.9340 - accuracy: 0.7143\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.8903 - accuracy: 0.8857\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.3696 - accuracy: 0.9143\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.9286\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 2.5817 - accuracy: 0.8889\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2600 - accuracy: 0.9571\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 8.2232 - accuracy: 0.6143\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.4663 - accuracy: 0.8000\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2571 - accuracy: 0.9714\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9429\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3674 - accuracy: 0.9714\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 14.3835 - accuracy: 0.8333\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 11.0894 - accuracy: 0.6197\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.6310 - accuracy: 0.8028\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.4534 - accuracy: 0.7606\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3128 - accuracy: 0.9014\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.6487 - accuracy: 0.9577\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9171 - accuracy: 0.8824\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9718\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 11.5195 - accuracy: 0.6197\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 4.3167 - accuracy: 0.6901\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.4651 - accuracy: 0.8873\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7608 - accuracy: 0.9718\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9437\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 3.3813 - accuracy: 0.8235\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.9859\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 10.7099 - accuracy: 0.5714\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.6205 - accuracy: 0.8429\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.9000\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8857\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9429\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9571\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.9571\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6534 - accuracy: 0.7778\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0609 - accuracy: 0.9571\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 10.0720 - accuracy: 0.5714\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.9340 - accuracy: 0.7143\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.8903 - accuracy: 0.8857\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.3696 - accuracy: 0.9143\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.9286\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.9571\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9714\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.0696 - accuracy: 0.9444\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.9714\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 8.2232 - accuracy: 0.6143\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.4663 - accuracy: 0.8000\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2571 - accuracy: 0.9714\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9429\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.9714\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9857\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 15.1445 - accuracy: 0.8333\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 11.0894 - accuracy: 0.6197\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 3.6310 - accuracy: 0.8028\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 1.4534 - accuracy: 0.7606\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.9014\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.6487 - accuracy: 0.9577\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9577\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.9859\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.7023 - accuracy: 0.8235\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9859\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 11.5195 - accuracy: 0.6197\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.3167 - accuracy: 0.6901\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.4651 - accuracy: 0.8873\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.7608 - accuracy: 0.9718\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9437\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.9859\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.9859\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.9926 - accuracy: 0.8824\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.9859\n",
            "Epoch 1/3\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 11.0994 - accuracy: 0.5714\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.4273 - accuracy: 0.8000\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5105 - accuracy: 0.9143\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 2.7845 - accuracy: 0.9444\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9286\n",
            "Epoch 1/3\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 11.1028 - accuracy: 0.6000\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 5.1653 - accuracy: 0.8143\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.7473 - accuracy: 0.8857\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 2.9407 - accuracy: 0.8889\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.3111 - accuracy: 0.9571\n",
            "Epoch 1/3\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 9.0550 - accuracy: 0.5571\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.6498 - accuracy: 0.8429\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1705 - accuracy: 0.9286\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 9.0145 - accuracy: 0.6111\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2691 - accuracy: 0.9571\n",
            "Epoch 1/3\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 11.4431 - accuracy: 0.6197\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.2149 - accuracy: 0.7887\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.7087 - accuracy: 0.7606\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 3.4687 - accuracy: 0.7059\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.9014\n",
            "Epoch 1/3\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 11.4219 - accuracy: 0.6761\n",
            "Epoch 2/3\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.5972 - accuracy: 0.7183\n",
            "Epoch 3/3\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2648 - accuracy: 0.9155\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.5932 - accuracy: 0.8235\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1494 - accuracy: 0.9296\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 11.0994 - accuracy: 0.5714\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 3.4273 - accuracy: 0.8000\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5105 - accuracy: 0.9143\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4057 - accuracy: 0.9000\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2614 - accuracy: 0.9429\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 3.3512 - accuracy: 0.8889\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1277 - accuracy: 0.9571\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 11.1028 - accuracy: 0.6000\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.1653 - accuracy: 0.8143\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.7473 - accuracy: 0.8857\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9162 - accuracy: 0.9143\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9857\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 2.3359 - accuracy: 0.8333\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9857\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 9.0550 - accuracy: 0.5571\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.6498 - accuracy: 0.8429\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 2.1705 - accuracy: 0.9286\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3667 - accuracy: 0.9429\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0054 - accuracy: 0.9429\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 10.2477 - accuracy: 0.7222\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 11.4431 - accuracy: 0.6197\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 3.2149 - accuracy: 0.7887\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.7087 - accuracy: 0.7606\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5004 - accuracy: 0.8873\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1144 - accuracy: 0.9577\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 1.0821 - accuracy: 0.7647\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 1.0000\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 11.4219 - accuracy: 0.6761\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 5.5972 - accuracy: 0.7183\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2648 - accuracy: 0.9155\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5169 - accuracy: 0.9014\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7016 - accuracy: 0.8873\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3986 - accuracy: 0.7647\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1002 - accuracy: 0.9718\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 11.0994 - accuracy: 0.5714\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 3.4273 - accuracy: 0.8000\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5105 - accuracy: 0.9143\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4057 - accuracy: 0.9000\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.9429\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9571\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0828 - accuracy: 0.9714\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 3.3748 - accuracy: 0.8333\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9857\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 11.1028 - accuracy: 0.6000\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 5.1653 - accuracy: 0.8143\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.7473 - accuracy: 0.8857\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.9162 - accuracy: 0.9143\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.9857\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2418 - accuracy: 0.9571\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2120 - accuracy: 0.9143\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.4708 - accuracy: 0.8889\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9857\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 5ms/step - loss: 9.0550 - accuracy: 0.5571\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.6498 - accuracy: 0.8429\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 2.1705 - accuracy: 0.9286\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.3667 - accuracy: 0.9429\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0054 - accuracy: 0.9429\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4103 - accuracy: 0.9286\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1384 - accuracy: 0.9714\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 12.1802 - accuracy: 0.6667\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1628 - accuracy: 0.9714\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 11.4431 - accuracy: 0.6197\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 3.2149 - accuracy: 0.7887\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.7087 - accuracy: 0.7606\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5004 - accuracy: 0.8873\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9577\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.9718\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0465 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.7265 - accuracy: 0.8235\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9859\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 11.4219 - accuracy: 0.6761\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 5.5972 - accuracy: 0.7183\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2648 - accuracy: 0.9155\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5169 - accuracy: 0.9014\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.7016 - accuracy: 0.8873\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2474 - accuracy: 0.9437\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9437\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.6341 - accuracy: 0.8824\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9437\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 5ms/step - loss: 11.8618 - accuracy: 0.5857\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.4589 - accuracy: 0.8143\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7803 - accuracy: 0.8714\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 1.8672 - accuracy: 0.8889\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9286\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 6ms/step - loss: 11.8528 - accuracy: 0.5857\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 3.4711 - accuracy: 0.8143\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3243 - accuracy: 0.8571\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.2423 - accuracy: 0.8889\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1964 - accuracy: 0.9143\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 5ms/step - loss: 10.8075 - accuracy: 0.5286\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.9460 - accuracy: 0.8714\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.3907 - accuracy: 0.8714\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 7.0053 - accuracy: 0.8333\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.9429\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 7ms/step - loss: 9.4773 - accuracy: 0.6620\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.9046 - accuracy: 0.7606\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.9014\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.3643 - accuracy: 0.8235\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9718\n",
            "Epoch 1/3\n",
            "3/3 [==============================] - 1s 6ms/step - loss: 12.5986 - accuracy: 0.6620\n",
            "Epoch 2/3\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.1251 - accuracy: 0.8451\n",
            "Epoch 3/3\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.5725 - accuracy: 0.8169\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.4236 - accuracy: 0.8235\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9577\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 1s 5ms/step - loss: 11.8618 - accuracy: 0.5857\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.4589 - accuracy: 0.8143\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7803 - accuracy: 0.8714\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9143\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5532 - accuracy: 0.9429\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 1.6003 - accuracy: 0.8889\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.0915 - accuracy: 0.9714\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 1s 6ms/step - loss: 11.8528 - accuracy: 0.5857\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.4711 - accuracy: 0.8143\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3243 - accuracy: 0.8571\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2670 - accuracy: 0.9429\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3047 - accuracy: 0.8714\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.2953 - accuracy: 0.9444\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2790 - accuracy: 0.8714\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 1s 8ms/step - loss: 10.8075 - accuracy: 0.5286\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 2.9460 - accuracy: 0.8714\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 2.3907 - accuracy: 0.8714\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.9429\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.1215 - accuracy: 0.9571\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 6.4308 - accuracy: 0.7222\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9143\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 1s 4ms/step - loss: 9.4773 - accuracy: 0.6620\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 3.9046 - accuracy: 0.7606\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.6722 - accuracy: 0.9014\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3822 - accuracy: 0.8873\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.2531 - accuracy: 0.9296\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.8306 - accuracy: 0.8235\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 1s 8ms/step - loss: 12.5986 - accuracy: 0.6620\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.1251 - accuracy: 0.8451\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.5725 - accuracy: 0.8169\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9437\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1674 - accuracy: 0.9014\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.4533 - accuracy: 0.8235\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9014\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 5ms/step - loss: 11.8618 - accuracy: 0.5857\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.4589 - accuracy: 0.8143\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.7803 - accuracy: 0.8714\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2087 - accuracy: 0.9143\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5532 - accuracy: 0.9429\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0544 - accuracy: 0.9714\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0609 - accuracy: 0.9714\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.3777 - accuracy: 0.7778\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.9714\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 6ms/step - loss: 11.8528 - accuracy: 0.5857\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.4711 - accuracy: 0.8143\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3243 - accuracy: 0.8571\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2670 - accuracy: 0.9429\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3047 - accuracy: 0.8714\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2639 - accuracy: 0.8714\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2679 - accuracy: 0.8571\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3352 - accuracy: 0.7222\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1657 - accuracy: 0.9143\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 6ms/step - loss: 10.8075 - accuracy: 0.5286\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.9460 - accuracy: 0.8714\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.3907 - accuracy: 0.8714\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.9429\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9571\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5505 - accuracy: 0.9143\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9143\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 6.8368 - accuracy: 0.7778\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.0977 - accuracy: 0.9571\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 7ms/step - loss: 9.4773 - accuracy: 0.6620\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 3.9046 - accuracy: 0.7606\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6722 - accuracy: 0.9014\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3822 - accuracy: 0.8873\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.2531 - accuracy: 0.9296\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0293 - accuracy: 0.9859\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9718\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 1.6680 - accuracy: 0.8235\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 8ms/step - loss: 12.5986 - accuracy: 0.6620\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 2.1251 - accuracy: 0.8451\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.5725 - accuracy: 0.8169\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1567 - accuracy: 0.9437\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1674 - accuracy: 0.9014\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2299 - accuracy: 0.9014\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2053 - accuracy: 0.9296\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.4505 - accuracy: 0.8235\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9718\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 1s 5ms/step - loss: 8.3248 - accuracy: 0.6705\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.6195 - accuracy: 0.8409\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 1.6792 - accuracy: 0.7841\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.7197 - accuracy: 0.9091\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.8636\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f2b34019890>,\n",
              "             param_grid={'batch_size': [15, 20, 25], 'epochs': [3, 5, 7]},\n",
              "             return_train_score=True)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBYAACMAaD9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a9f08d51-865d-4b7c-f204-733a1b542a46"
      },
      "source": [
        "res = pd.DataFrame(validator_red_drop_nn.cv_results_)\n",
        "res.pivot_table(index=[\"param_epochs\",\"param_batch_size\"],values=['mean_train_score',\"mean_test_score\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>param_epochs</th>\n",
              "      <th>param_batch_size</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
              "      <th>15</th>\n",
              "      <td>0.818301</td>\n",
              "      <td>0.914809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.794771</td>\n",
              "      <td>0.934769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.851634</td>\n",
              "      <td>0.943058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
              "      <th>15</th>\n",
              "      <td>0.852288</td>\n",
              "      <td>0.974406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.794771</td>\n",
              "      <td>0.982938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.840523</td>\n",
              "      <td>0.931710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">7</th>\n",
              "      <th>15</th>\n",
              "      <td>0.852288</td>\n",
              "      <td>0.980080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.818954</td>\n",
              "      <td>0.974487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.784967</td>\n",
              "      <td>0.962938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               mean_test_score  mean_train_score\n",
              "param_epochs param_batch_size                                   \n",
              "3            15                       0.818301          0.914809\n",
              "             20                       0.794771          0.934769\n",
              "             25                       0.851634          0.943058\n",
              "5            15                       0.852288          0.974406\n",
              "             20                       0.794771          0.982938\n",
              "             25                       0.840523          0.931710\n",
              "7            15                       0.852288          0.980080\n",
              "             20                       0.818954          0.974487\n",
              "             25                       0.784967          0.962938"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTNj7Rmxsk8E",
        "outputId": "34e119aa-5336-42a9-cdf6-bff5852f7b6d"
      },
      "source": [
        "print(\"Best hyperparameters: \",validator_red_drop_nn.best_params_)\n",
        "print(\"Best CV accuracy: \",validator_red_drop_nn.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'batch_size': 15, 'epochs': 5}\n",
            "Best CV accuracy:  0.8522875785827637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVN0HTgw12_y",
        "outputId": "733b2bda-e45d-458b-b626-e309a9779398"
      },
      "source": [
        "best_model = validator_red_drop_nn.best_estimator_.model\n",
        "loss,accuracy = best_model.evaluate(testx_reduced,test_y)\n",
        "print(\"Accuracy on Test: \", accuracy)\n",
        "print(\"Loss on Test: \", loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3096 - accuracy: 0.8077\n",
            "Accuracy on Test:  0.807692289352417\n",
            "Loss on Test:  0.3096109628677368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdqVoVgg0IwT"
      },
      "source": [
        "The Dropout doesn't seem to increase the accuracy using a MLP after dimensionality reduction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTcjaTMj0HxT"
      },
      "source": [
        "# **Convolutional Neural Networks (CNNs)**\n",
        "CNNs are maybe the most popular DL architecture. They are very computationally efficient. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-8I159SP7DN"
      },
      "source": [
        "## *CNN with 1D Convolutional layer*\n",
        "A 1D conv layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. The dataset has been encoded in 1D vector and, to avoid RAM problems, I reduced the dimension of the vector to the first 5K genes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDFSQmQqHJbn"
      },
      "source": [
        "#transform train set into an array (for RAM problems I kept only the first 5k genes)\n",
        "trainx_cnn = train_x.iloc[:,:5000].to_numpy()\n",
        "#add 1 dim to the input vector\n",
        "trainx_cnn = numpy.reshape(trainx_cnn,(88,5000,1))\n",
        "trainx_cnn = trainx_cnn.astype(\"float32\")\n",
        "\n",
        "#let's do the same for the test set\n",
        "testx_cnn = test_x.iloc[:,:5000].to_numpy()\n",
        "testx_cnn = numpy.reshape(testx_cnn,(26,5000,1))\n",
        "testx_cnn = testx_cnn.astype(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqFgILQhIfij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ce8e57-cb5b-433b-9da3-d9e043a91c24"
      },
      "source": [
        "def make_model(dense_layer_sizes, filters, kernel_size):\n",
        "  set_seed(432)\n",
        "  model = models.Sequential()\n",
        "  model.add(keras.layers.Conv1D(filters,kernel_size=kernel_size,input_shape=(5000,1)))\n",
        "  model.add(keras.layers.Activation('relu'))\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.MaxPooling1D(1))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(dense_layer_sizes, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "my_classifier = KerasClassifier(make_model, batch_size=20)\n",
        "validator_1Dconv = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [32,64,128],\n",
        "                                     'epochs': [5,7],\n",
        "                                     'filters': [32,64,128],\n",
        "                                     'kernel_size': [3,5,7]},\n",
        "                         scoring='accuracy',\n",
        "                         n_jobs=1,return_train_score=True)\n",
        "validator_1Dconv.fit(trainx_cnn, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "4/4 [==============================] - 2s 282ms/step - loss: 2.7796 - accuracy: 0.7143\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 1s 277ms/step - loss: 2.3726 - accuracy: 0.8000\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 1s 277ms/step - loss: 1.0051 - accuracy: 0.9286\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 1s 279ms/step - loss: 0.6025 - accuracy: 0.9429\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 1s 275ms/step - loss: 0.1243 - accuracy: 0.9571\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 1s 277ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 1s 271ms/step - loss: 0.0207 - accuracy: 0.9857\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 2s 309ms/step - loss: 5.6329 - accuracy: 0.6429\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 1s 276ms/step - loss: 3.9020 - accuracy: 0.7143\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 1s 277ms/step - loss: 1.3988 - accuracy: 0.9286\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 1s 281ms/step - loss: 1.5685 - accuracy: 0.8286\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 1s 276ms/step - loss: 0.4748 - accuracy: 0.9429\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 1s 281ms/step - loss: 0.3699 - accuracy: 0.9143\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 1s 278ms/step - loss: 0.0978 - accuracy: 0.9714\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 2s 288ms/step - loss: 1.4337 - accuracy: 0.7714\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 1s 287ms/step - loss: 0.7764 - accuracy: 0.9143\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 1s 293ms/step - loss: 0.6588 - accuracy: 0.9286\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 1s 287ms/step - loss: 0.1928 - accuracy: 0.9286\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 1s 284ms/step - loss: 0.0785 - accuracy: 0.9714\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 1s 288ms/step - loss: 0.0552 - accuracy: 0.9857\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 1s 282ms/step - loss: 2.5333e-04 - accuracy: 1.0000\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 2s 280ms/step - loss: 3.0053 - accuracy: 0.6620\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 1s 295ms/step - loss: 1.6894 - accuracy: 0.9155\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 1s 283ms/step - loss: 0.6689 - accuracy: 0.9577\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 1s 295ms/step - loss: 0.0581 - accuracy: 0.9718\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 1s 290ms/step - loss: 0.1222 - accuracy: 0.9718\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 1s 285ms/step - loss: 0.1322 - accuracy: 0.9577\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 1s 286ms/step - loss: 0.2769 - accuracy: 0.9859\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 2s 303ms/step - loss: 3.4199 - accuracy: 0.6197\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 1s 290ms/step - loss: 1.6544 - accuracy: 0.9155\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 1s 284ms/step - loss: 1.2409 - accuracy: 0.9296\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 1s 284ms/step - loss: 0.8431 - accuracy: 0.9296\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 1s 285ms/step - loss: 0.2979 - accuracy: 0.9577\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 1s 292ms/step - loss: 0.0223 - accuracy: 0.9859\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 1s 302ms/step - loss: 1.4663e-04 - accuracy: 1.0000\n",
            "Epoch 1/7\n",
            "5/5 [==============================] - 2s 302ms/step - loss: 1.7462 - accuracy: 0.7614\n",
            "Epoch 2/7\n",
            "5/5 [==============================] - 2s 296ms/step - loss: 2.6027 - accuracy: 0.8295\n",
            "Epoch 3/7\n",
            "5/5 [==============================] - 2s 296ms/step - loss: 0.8728 - accuracy: 0.8864\n",
            "Epoch 4/7\n",
            "5/5 [==============================] - 1s 291ms/step - loss: 0.7319 - accuracy: 0.8977\n",
            "Epoch 5/7\n",
            "5/5 [==============================] - 1s 286ms/step - loss: 0.2324 - accuracy: 0.9432\n",
            "Epoch 6/7\n",
            "5/5 [==============================] - 1s 291ms/step - loss: 0.1095 - accuracy: 0.9659\n",
            "Epoch 7/7\n",
            "5/5 [==============================] - 1s 291ms/step - loss: 0.0446 - accuracy: 0.9886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f07ca503950>,\n",
              "             n_jobs=1,\n",
              "             param_grid={'dense_layer_sizes': [64], 'epochs': [7],\n",
              "                         'filters': [64], 'kernel_size': [3]},\n",
              "             return_train_score=True, scoring='accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlELwELWaLDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "1135fc03-5530-4c21-9cf4-0758655d4b5b"
      },
      "source": [
        "res = pd.DataFrame(validator_1Dconv.cv_results_)\n",
        "res.pivot_table(index=[\"param_epochs\",\"param_filters\",\"param_dense_layer_sizes\",\"param_kernel_size\"],values=['mean_train_score',\"mean_test_score\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>param_epochs</th>\n",
              "      <th>param_filters</th>\n",
              "      <th>param_dense_layer_sizes</th>\n",
              "      <th>param_kernel_size</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">32</th>\n",
              "      <th>64</th>\n",
              "      <th>50</th>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.900201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <th>50</th>\n",
              "      <td>0.830719</td>\n",
              "      <td>0.897545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">64</th>\n",
              "      <th>64</th>\n",
              "      <th>50</th>\n",
              "      <td>0.784967</td>\n",
              "      <td>0.922978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <th>50</th>\n",
              "      <td>0.738562</td>\n",
              "      <td>0.906117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">5</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">32</th>\n",
              "      <th>64</th>\n",
              "      <th>50</th>\n",
              "      <td>0.796078</td>\n",
              "      <td>0.948732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <th>50</th>\n",
              "      <td>0.805882</td>\n",
              "      <td>0.962978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">64</th>\n",
              "      <th>64</th>\n",
              "      <th>50</th>\n",
              "      <td>0.818954</td>\n",
              "      <td>0.960040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <th>50</th>\n",
              "      <td>0.839869</td>\n",
              "      <td>0.963058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">7</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">32</th>\n",
              "      <th>64</th>\n",
              "      <th>50</th>\n",
              "      <td>0.851634</td>\n",
              "      <td>0.994326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <th>50</th>\n",
              "      <td>0.839869</td>\n",
              "      <td>0.988571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">64</th>\n",
              "      <th>64</th>\n",
              "      <th>50</th>\n",
              "      <td>0.839869</td>\n",
              "      <td>0.997143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <th>50</th>\n",
              "      <td>0.839869</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                      mean_test_score  mean_train_score\n",
              "param_epochs param_filters param_dense_layer_sizes param_kernel_size                                   \n",
              "3            32            64                      50                        0.796078          0.900201\n",
              "                           128                     50                        0.830719          0.897545\n",
              "             64            64                      50                        0.784967          0.922978\n",
              "                           128                     50                        0.738562          0.906117\n",
              "5            32            64                      50                        0.796078          0.948732\n",
              "                           128                     50                        0.805882          0.962978\n",
              "             64            64                      50                        0.818954          0.960040\n",
              "                           128                     50                        0.839869          0.963058\n",
              "7            32            64                      50                        0.851634          0.994326\n",
              "                           128                     50                        0.839869          0.988571\n",
              "             64            64                      50                        0.839869          0.997143\n",
              "                           128                     50                        0.839869          1.000000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJUUsygUKaxw",
        "outputId": "f59160ce-2595-4834-e605-b0303d90f862"
      },
      "source": [
        "print(\"Best hyperparameters: \",validator_1Dconv.best_params_)\n",
        "print(\"Best CV accuracy: \",validator_1Dconv.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'dense_layer_sizes': 64, 'epochs': 7, 'filters': 64, 'kernel_size': 3}\n",
            "Best CV accuracy:  0.8516339869281045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOtGrQqeKeCn",
        "outputId": "ef2468aa-c278-408f-a615-e0fc601080fa"
      },
      "source": [
        "best_1Dconv_model = validator_1Dconv.best_estimator_.model\n",
        "loss, accuracy = best_1Dconv_model.evaluate(testx_cnn, test_y,verbose=0)\n",
        "print(\"Accuracy on Test: \",accuracy)\n",
        "print(\"Loss on Test: \",loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test:  0.8846153616905212\n",
            "Loss on Test:  2.524669885635376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haSI6jsR2gzv"
      },
      "source": [
        "## *CNN with input matrix*\n",
        "In this case, the CNN has a matrix 2D picture format-like in input (instead of a vector), as the most common CNNs, and a 2D convolutional layer. The output is then passed to a 2D maxpooling layer, a Fully Connected (FC) layer, and a prediction layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k-d9iyt3lDX"
      },
      "source": [
        "#transform train set into an array\n",
        "trainx_cnn = train_x.to_numpy()\n",
        "#add 23 zeros to the 1 axis of the array\n",
        "trainx_cnn = numpy.concatenate((trainx_cnn,numpy.zeros((88,23))),axis=1)\n",
        "#reshape the array in order to obtain an array of (88,223,100,1) dim\n",
        "trainx_cnn = numpy.reshape(trainx_cnn,(-1,223,100,1))\n",
        "trainx_cnn = trainx_cnn.astype(\"float32\")\n",
        "\n",
        "#let's do the same for the test set\n",
        "testx_cnn = test_x.to_numpy()\n",
        "testx_cnn = numpy.concatenate((testx_cnn,numpy.zeros((26,23))),axis=1)\n",
        "testx_cnn = numpy.reshape(testx_cnn,(-1,223,100,1))\n",
        "testx_cnn = testx_cnn.astype(\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo9J6btM3lvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6fc2ba-acba-4e10-cd8a-6f9e83b90265"
      },
      "source": [
        "def make_model(dense_layer_sizes, filters, kernel_size):\n",
        "  set_seed(432)\n",
        "  model = models.Sequential()\n",
        "  model.add(keras.layers.Conv2D(filters,kernel_size=kernel_size,strides=(5,5),input_shape=(223,100,1)))\n",
        "  model.add(keras.layers.Activation('relu'))\n",
        "  model.add(keras.layers.MaxPooling2D(2, 2))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(dense_layer_sizes, activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "my_classifier = KerasClassifier(make_model, batch_size=25)\n",
        "validator_matrix = GridSearchCV(my_classifier,\n",
        "                         param_grid={'dense_layer_sizes': [64,128],\n",
        "                                     'epochs': [5,7],\n",
        "                                     'filters': [32,64,128],\n",
        "                                     'kernel_size': [(7, 7), (10, 10), (15, 15)]},\n",
        "                         scoring='accuracy',\n",
        "                         n_jobs=1,return_train_score=True)\n",
        "\n",
        "validator_matrix.fit(trainx_cnn, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 42ms/step - loss: 1.5067 - accuracy: 0.4571\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.8145 - accuracy: 0.5571\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.8748 - accuracy: 0.5571\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.4484 - accuracy: 0.7571\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.4506 - accuracy: 0.7286\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.3038 - accuracy: 0.9143\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.2887 - accuracy: 0.9000\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 48ms/step - loss: 1.2370 - accuracy: 0.5429\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.9676 - accuracy: 0.6429\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.6770 - accuracy: 0.6286\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.3296 - accuracy: 0.8571\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.3856 - accuracy: 0.8429\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.3326 - accuracy: 0.9000\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.2169 - accuracy: 0.9286\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 42ms/step - loss: 1.4523 - accuracy: 0.4714\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.9054 - accuracy: 0.5714\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.5547 - accuracy: 0.6571\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.5165 - accuracy: 0.7429\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2915 - accuracy: 0.8571\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.3408 - accuracy: 0.9000\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2397 - accuracy: 0.9286\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 46ms/step - loss: 1.0503 - accuracy: 0.4225\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.5865 - accuracy: 0.6761\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.4711 - accuracy: 0.7887\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.4362 - accuracy: 0.7887\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.4254 - accuracy: 0.8169\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 0.2002 - accuracy: 0.9296\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.3105 - accuracy: 0.8592\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 41ms/step - loss: 0.7388 - accuracy: 0.5070\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 0.7791 - accuracy: 0.5915\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.4628 - accuracy: 0.8028\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2671 - accuracy: 0.9296\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 0.2527 - accuracy: 0.9014\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 0.1475 - accuracy: 0.9718\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.1347 - accuracy: 0.9437\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 50ms/step - loss: 1.2520 - accuracy: 0.4286\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.5810 - accuracy: 0.6286\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.5661 - accuracy: 0.6714\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.3934 - accuracy: 0.8429\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.3089 - accuracy: 0.9000\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.2334 - accuracy: 0.9143\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.1926 - accuracy: 0.9429\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 52ms/step - loss: 1.1743 - accuracy: 0.4429\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.7046 - accuracy: 0.6571\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.7324 - accuracy: 0.6000\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.4417 - accuracy: 0.8143\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.3244 - accuracy: 0.9000\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.2820 - accuracy: 0.8714\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.2231 - accuracy: 0.9286\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 52ms/step - loss: 0.7872 - accuracy: 0.5571\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.7376 - accuracy: 0.7000\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 0.3760 - accuracy: 0.8143\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 0.2292 - accuracy: 0.9000\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1335 - accuracy: 0.9571\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 0.1640 - accuracy: 0.9286\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 0.1155 - accuracy: 0.9571\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 54ms/step - loss: 1.3323 - accuracy: 0.4225\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 0.7054 - accuracy: 0.5211\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.6329 - accuracy: 0.6901\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.5087 - accuracy: 0.8451\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 0.4212 - accuracy: 0.8451\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.3232 - accuracy: 0.8873\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.2554 - accuracy: 0.9296\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 56ms/step - loss: 1.1815 - accuracy: 0.4507\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 0.5099 - accuracy: 0.7606\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.5132 - accuracy: 0.6901\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.5480 - accuracy: 0.6197\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.3803 - accuracy: 0.8310\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.3326 - accuracy: 0.8732\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.1943 - accuracy: 0.9577\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 69ms/step - loss: 1.2477 - accuracy: 0.5286\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.5138 - accuracy: 0.6857\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.4194 - accuracy: 0.8857\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2718 - accuracy: 0.9429\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2056 - accuracy: 0.9286\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1574 - accuracy: 0.9714\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1213 - accuracy: 0.9714\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 75ms/step - loss: 1.0663 - accuracy: 0.5000\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.5346 - accuracy: 0.7571\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.5013 - accuracy: 0.8286\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3437 - accuracy: 0.9000\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.2435 - accuracy: 0.9286\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.2032 - accuracy: 0.9571\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1491 - accuracy: 0.9571\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 72ms/step - loss: 0.7916 - accuracy: 0.5857\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.6424 - accuracy: 0.7857\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.3866 - accuracy: 0.7714\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.1658 - accuracy: 0.9571\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.1484 - accuracy: 0.9571\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.1429 - accuracy: 0.9429\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.0761 - accuracy: 0.9571\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 73ms/step - loss: 1.3026 - accuracy: 0.4648\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.7481 - accuracy: 0.5070\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5689 - accuracy: 0.8169\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.4233 - accuracy: 0.8732\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.3403 - accuracy: 0.9014\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.2462 - accuracy: 0.9296\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.1899 - accuracy: 0.9296\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 1s 77ms/step - loss: 1.1535 - accuracy: 0.5211\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.5475 - accuracy: 0.7606\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 0.6817 - accuracy: 0.6338\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5048 - accuracy: 0.7042\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.3620 - accuracy: 0.8592\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2648 - accuracy: 0.8873\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 0.1569 - accuracy: 0.9577\n",
            "Epoch 1/7\n",
            "4/4 [==============================] - 1s 40ms/step - loss: 1.0608 - accuracy: 0.4432\n",
            "Epoch 2/7\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 0.6444 - accuracy: 0.6364\n",
            "Epoch 3/7\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.4699 - accuracy: 0.8636\n",
            "Epoch 4/7\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.3349 - accuracy: 0.8750\n",
            "Epoch 5/7\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 0.2830 - accuracy: 0.8977\n",
            "Epoch 6/7\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 0.2744 - accuracy: 0.9091\n",
            "Epoch 7/7\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.2076 - accuracy: 0.9205\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7efbd7e8f810>,\n",
              "             n_jobs=1,\n",
              "             param_grid={'dense_layer_sizes': [64], 'epochs': [7],\n",
              "                         'filters': [64],\n",
              "                         'kernel_size': [(7, 7), (10, 10), (15, 15)]},\n",
              "             return_train_score=True, scoring='accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wfelH7Qbvhx"
      },
      "source": [
        "res = pd.DataFrame(validator_matrix.cv_results_)\n",
        "res.pivot_table(index=[\"param_epochs\",\"param_filters\",\"param_dense_layer_sizes\",\"param_kernel_size\"],values=['mean_train_score',\"mean_test_score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsptTNT58yyH",
        "outputId": "5731fb79-861f-4932-aa79-b934cfdf0114"
      },
      "source": [
        "print(\"Best hyperparameters: \",validator_matrix.best_params_)\n",
        "print(\"Best CV accuracy: \",validator_matrix.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'dense_layer_sizes': 64, 'epochs': 7, 'filters': 64, 'kernel_size': (7, 7)}\n",
            "Best CV accuracy:  0.8751633986928106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XezW6S6Y869U",
        "outputId": "559ad7aa-3b90-424c-b47b-507d64fc7149"
      },
      "source": [
        "best_matr_model = validator_matrix.best_estimator_.model\n",
        "loss, accuracy = best_matr_model.evaluate(testx_cnn, test_y,verbose=0)\n",
        "print(\"Accuracy on Test: \",accuracy)\n",
        "print(\"Loss on Test: \",loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test:  0.9230769276618958\n",
            "Loss on Test:  0.3179877996444702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd3NvQNgULLD"
      },
      "source": [
        "# **Results and Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kie8L4vcHv5S"
      },
      "source": [
        "According to the results obtained on both training and test set, the models with the highest accuracies are the MLP after dimensionality reduction and the CNN with input matrix. In the first one the accuracy on the training set has reached a peak of 96% and 92% on the test set, while in the second one 92% on both the training and the test set, even if in the validation set has reached 87%. However, even with these two models and consistently with the other models, a problem of overfitting is still present. This problem can be reduced, for example using Dropout or BatchNormalization, but the source reason of overfitting is the low number of samples (examples). Thus, the only way to strongly reduce the overfitting is to increase the number of samples: indeed, with a very high number of features (genes) and low number of examples (patients), the overfitting is an expected phenomena."
      ]
    }
  ]
}